\chapter{Odwzorowania dodatnie na algebrach macierzowych}
\label{chp:M3notes}

Po wykazaniu w poprzednim rozdziale znaczenia odwzorowań dodatnich dla kryterium
splątania w najogólniejszym z fizycznego punktu widzenia przypadku
iniektywnych algebr von Neumanna, przejdźmy do analizy samych odwzorowań dodatnich.

Powiemy że dodatnie odwzorowanie $S$ na algebrze macierzy $M_{n}$ spełnia nierówność
Kadisona-Schwarza, jeśli
\begin{linenomath*}
 \begin{equation}
\label{eq:SchwarzInequality}
    S(X^{*}) \, S(X) \: \leq \: S(X^{*} X),
 \end{equation}
\end{linenomath*}
dla każdej macierzy $X \in M_{n}$.
W ogólności dodatnie odwzorowanie $S$, które zachowuje macierz jednostkową,
tzn. $S(\mathbf{1}) = \mathbf{1}$,
spełnia nierówność Kadisona-Schwarza dla macierzy normalnych,
to jest takich macierzy
$X$, dla których  $X X^{*} = X^{*} X$
\cite{choi1980some}.
Odwzorowanie dodatnie, które spełnia tę nierówność dla wszystkich macierzy
nazwiemy \emph{odwzorowaniem Schwarza} \cite{robertson1983schwarz}.
Przypomnijmy także, że odwzorowanie $S$ nazywamy bistochastycznym,
jeśli jest ono dodatnie oraz zachowuje ślad i macierz jednostkową,
tzn.
$S(\mathbf{1}) = \mathbf{1}$
i
$\text{Tr} \, S(X) = \text{Tr} \, X$
dla każdego $X \in M_{n}$.


\section{Charakterystyka odwzorowań algebry $M_{2}$}
\label{sec:M2notes}

W tym podrozdziale przestawimy wraz z dowodami rezultaty opublikowane w pracy
\cite{miller2015stable},
który stanowi alternatywny dowód twierdzenia udowodnionego przez
E.\,St{\o}rmera \cite{stormer1963positive}
i S.\,L.\,Woronowicza \cite{woronowicz1976positive},
podającego pełną charakterystykę odwzorowań dodatnich na algebrze $M_{2}$.

Niech $\mu,\nu = 0,1,2,3$ oraz $i,j = 1,2,3$.
Niech także $\left\{ \sigma_{\mu} \right\}_{\mu=0}^{3}$
będzie bazą w  $M_{2}$, taką że
$\sigma_{0} = \tfrac{1}{\sqrt{2}}  \mathbf{1}$ oraz
$\sigma_{i}$
są znormalizowanymi macierzami Pauliego dla $i=1,2,3$:
\begin{linenomath*}
 \begin{equation}
 \sigma_{1} = \frac{1}{\sqrt{2}} \begin{pmatrix}
        0 & 1 \\ 1 & 0
           \end{pmatrix},
\quad
\sigma_{2} = \frac{1}{\sqrt{2}}  \begin{pmatrix}
        0 & -i \\ i & 0
           \end{pmatrix},
\quad
\sigma_{3} = \frac{1}{\sqrt{2}}  \begin{pmatrix}
        1 & 0 \\ 0 & -1
           \end{pmatrix}.
 \end{equation}
\end{linenomath*}
Macierz hermitowska $\sigma(a) \in M_{2}$,
$\sigma(a) = \sum_{\mu =0}^{3} a_{\mu} \sigma_{\mu}$,
dla $a = (a_{\mu}) = (a_{0}, \vec{a}) \in \mathbb{R}^{4}$,
jest dodatnia, wtedy i tylko wtedy, gdy
$x \in L_{4}$,
gdzie
\begin{linenomath*}
 \begin{equation}
L_{4} = \left \{
            a \in \mathbb{R}^{4} :
            a_{0} \geq \sqrt{(a_{1})^{2} + (a_{2})^{2} + (a_{3})^{2}} \,
        \right \}.
 \end{equation}
\end{linenomath*}
Niech $\vec{a} = (a_{1}, a_{2}, a_{3}) \in \mathbb{R}^{3}$,
a także niech $||\vec{a}||_{2}$ będzie normą euklidesową wektora.

Jeśli $S: M_{2} \rightarrow M_{2}$ jest odwzorowaniem hermitowskim,
możemy zdefiniować macierz rzeczywistą $\pi(S) \in M_{4}(\mathbb{R})$ poprzez
\begin{linenomath*}
 \begin{equation}
\label{def:PiofSiso}
  \sigma(\pi(S) x) = S \sigma(a), \quad x \in \mathbb{R}^{4}
 \end{equation}
\end{linenomath*}
lub też $\pi(S)_{\mu \nu} = \text{Tr} \sigma_{\mu} S(\sigma_{\nu})$.
Rzecz jasna, dla dwóch takich odwzorowań: $S_{1}, S_{2}$, mamy
$\pi(S_{1} S_{2}) = \pi(S_{1}) \pi(S_{2})$
oraz $\pi(S^{-1}) = \pi(S)^{-1}$, jeśli tylko $S^{-1}$ istnieje.
Odwzorowanie $S$ jest dodatnie, wtedy i tylko wtedy, gdy
$\pi(S)$ przekształca zbiór $L_{4}$ w siebie.

\index{idx}{stożek wypukły}
Przypomnijmy, że zbiór $\mathcal{P} \subset \mathbb{R}^{n}$
jest z definicji stożkiem (wypukłym), jeśli
$\alpha x + \beta y \in \mathcal{P}$,
gdy tylko $x, y \in \mathcal{P}$ i $\alpha, \beta \geq 0$.
Element $x \in \mathcal{P}$ nazywamy ekstremalnym, gdy
z faktu, że $x - y \in \mathcal{P}$,
dla pewnego $y \in \mathcal{P}$,
wynika $y = \alpha x$, $\alpha \geq 0$.
Zbiór elementów ekstremalnych stożka $\mathcal{P}$ oznaczmy przez
Ext\,$\mathcal{P}$.

Stożek $L_{4}$ nazwiemy \emph{stożkiem Lorentza} w $\mathbb{R}^{4}$.
Topologiczny brzeg $\partial L_{4}$ stożka $L_{4}$ to zbiór
$\partial L_{4} = \{ a \in L_{4} : a_{0} = || \vec{a} ||_{2} \}$.
Prawdą jest, że w tym przypadku $\partial L_{4} = \text{Ext}\,L_{4}$.
Symbolem $\Theta(L_{4})$ oznaczmy grupę operatorów z $M_{4}(\mathbb{R})$,
które odwzorowują $\partial L_{4}$ na siebie.
Z twierdzenia 2.4 w pracy
\cite{loewy1975positive},
dla każdego $\tilde{A} \in \Theta(L_{4})$,
$\tilde{A} = r \tilde{O}$,
gdzie $r > 0$, a $\tilde{O}$ należy do
podgrupy grupy Lorentza $\mathrm{O}^{+}(1,3)$.
Niech $\rho: SL_{2}(\mathbb{C}) \rightarrow \text{SO}^{+}(1,3)$
będzie standardowym izomorfizmem pomiędzy grupą macierzy z $M_{2}$
o wyznaczniku równym jedności, a elementami grupy $\text{SO}^{+}(1,3) \subset \text{O}^{+}(1,3)$
(por. np. G.\,L.\,Naber, \emph{The Geometry of Minkowski Spacetime}
\cite{Naber1992}).
Z definicji
\begin{linenomath*}
 \begin{equation}
\sigma(\rho(V) x) = V^{*} \sigma(a) V,
 \end{equation}
\end{linenomath*}
dla dowolnego $V \in SL_{2}(\mathbb{C})$ i $x \in \mathbb{R}^{4}$.
Zatem dla odwzorowania $S_{V}: M_{2} \rightarrow M_{2}$,
takiego, że $S_{V} A = V^{*} A V$, $V \in \text{SL}_{2}(\mathbb{C})$,
mamy $\rho(V) = \pi(S_{V})$.
Jasne jest, że każdy element grupy $\text{O}^{+}(1,3)$,
który nie należy do  $\text{SO}^{+}(1,3)$,
jest równy $\Lambda J$,
gdzie $\Lambda \in \text{SO}^{+}(1,3)$ oraz $J$ jest macierzą diagonalną,
$J = \mathrm{diag}(1,1,-1,1)$.
Taka postać macierzy $J$ została wybrana tylko z uwagi na wygodę zastosowania
w dowodzie twierdzenia poniżej.
Istotne jest jedynie to, że
$J \in \text{O}^{+}(1,3)$ i $\mathrm{det} \, J = -1$.

\begin{Theorem}
\label{thm:PositiveMapsOnM2}
    Niech $S:M_{2} \rightarrow M_{2}$ będzie odwzorowaniem dodatnim.
    Wówczas $S = \Lambda_{1}  + \Lambda_{2} \circ t$,
    gdzie odwzorowania
    $\Lambda_{1}, \Lambda_{2}:M_{2} \rightarrow M_{2}$
    są kompletnie dodatnie, a
    $t: M_{2} \rightarrow M_{2}$ oznacza transpozycję macierzy na $M_{2}$.
\end{Theorem}

\begin{proof}
\label{RandomLabel:875919}
  Odwzorowanie $S$ działa na wektory bazowe $\sigma_{\mu}$ poprzez
\begin{linenomath*}
 \begin{equation}
 S \sigma_{\mu} = \sum_{\nu = 0}^{3} \pi(S)_{\mu \nu} \sigma_{\nu}.
 \end{equation}
\end{linenomath*}
Załóżmy na początek, że $\pi(S) \in \Theta(L_{4})$.
Jeżeli $\pi(S) = \rho(V) \in \text{SO}^{+}(1,3)$,
$V \in SL_{2}(\mathbb{C})$,
wówczas
$S \sigma(a) = \sum_{\mu = 0}^{3}  a_{\mu} V^{*} \sigma_{\mu} V =
 V^{*} \sigma(a) V$,
 $x \in \mathbb{R}^{4}$,
a więc $S$ jest kompletnie dodatnie,
skoro każda macierz $A \in M_{2}$ jest kombinacją liniową macierzy hermitowskich.
Z drugiej strony,
jeśli $\pi(S) = \rho(V) J$,
to
\begin{linenomath*}
 \begin{multline}
\label{RandomLabel:830200}
S \sigma(a) =
  a_{0} \, V^{*}V + a_{1} \, V^{*} \sigma_{1} V -
  a_{2} \, V^{*} \sigma_{2} V +
  a_{3} \, V^{*} \sigma_{3} V = \\
  = a_{0} \, V^{*}V + a_{1} \, V^{*} \sigma_{1}^{t} V +
  a_{2} \, V^{*} \sigma_{2}^{t} V +
  a_{3} \, V^{*} \sigma_{3}^{t} V =
  V^{*} \sigma(a)^{t} V,
 \end{multline}
\end{linenomath*}
ponieważ $\sigma_{2}^{t} = - \sigma_{2}$.
Zatem, $S$ jest złożeniem transpozycji i odwzorowania kompletnie dodatniego
(tzn. $S$  jest kompletnie \emph{ko}dodatnie).
Z kolei załóżmy, że $\pi(S) \in \delta(L_{4})$,
gdzie
$
 \delta(L_{4}) = \left \{ u w^{t}:
 \, u, w \in \partial L_{4}  \right \}
$,
jest zbiorem operatorów rzędu 1, zachowujących brzeg stożka $\partial L_{4}$.
Na mocy  lematu 3.2 z pracy
\cite{loewy1975positive},
$S$ jest ekstremalnym odwzorowaniem dodatnim, takim że $\text{rank}\,S = 1$,
co implikuje, że $S$ musi być kompletnie dodatnie.

Jeżeli $\pi(S)$ zachowuje stożek Lorentza,
na mocy \mbox{twierdzenia 4.5} \cite{loewy1975positive},
$\pi(S) \in \text{conv} \left ( \Theta(L_{4}) \cup \delta(L_{4}) \right)$.
Innymi słowy każdy operator zachowujący stożek Lorentza $L_{4}$
jest wypukłą kombinacją tych operatorów, które dodatkowo zachowują brzeg stożka.
Z tego, co zostało pokazane powyżej, wynika, iż
\begin{linenomath*}
 \begin{equation}
\label{RandomLabel:587827}
    S = \Lambda_{1} + \Lambda_{2} \circ t,
 \end{equation}
\end{linenomath*}
gdzie $\Lambda_{1}$, $\Lambda_{2}$
są odwzorowaniami kompletnie dodatnimi.
\end{proof}

Twierdzenie \ref{thm:PositiveMapsOnM2} podaje alternatywny dowód
znanego rezultatu \cite{stormer1963positive, woronowicz1976positive}.
Jednak, o ile metody wykorzystywane przez autorów tamtych prac
polegają w głównej mierze na algebraicznych manipulacjach macierzami,
o tyle wprowadzenie odwzorowania $\pi$ pozwoliło nam
na podkreślenie istotnych dla problemu geometrycznych własności stożków
zawartych w  $\mathbb{R}^{4}$.
Należy również dodać, że podobne metody zostały już wprowadzone wcześniej
\cite{leinaas2006geometrical}.
Dzięki temu udało się otrzymać pełną charakteryzację odwzorowań dodatnich
na $M_{2}$,
której źródeł należy się doszukiwać w szczególnej symetrii stożka
$L_{4}$, reprezentującego macierze dodatnie z $M_{2}$.
W tym najprostszym przypadku mogliśmy wykorzystać znany wcześniej wynik
\cite{loewy1975positive}, dotyczący
opisu operatorów zachowujących stożek $L_{4}$.
Jak zobaczymy poniżej, jeśli przejdziemy do kolejnego przypadku
odwzorowań dodatnich na $M_{3}$,
stożek wektorów $a \in \mathbb{R}^{9}$ takich że
$\lambda(a) =  \sum_{\mu=0}^{8} a_{\mu} \lambda_{\mu}$
jest dodatnią macierzą z $M_{3}$,
dla dowolnej bazy $(\lambda_{\mu})_{\mu=0}^{8}$ przestrzeni liniowej $M_{3}$,
nie jest już stożkiem Lorentza $L_{9}$ \cite{goyal2011geometry}.
Dlatego też, kiedy przyjdzie nam zająć się tym przypadkiem,
będziemy musieli poszerzyć metody analizy o dodatkowe narzędzia.


Naturalne pytanie, które pojawia się w kontekście
twierdzenia \ref{thm:PositiveMapsOnM2},
to kiedy dodatnie odwzorowania na $M_{2}$ jest również kompletnie dodatnie.

\begin{Theorem}
\label{thm:MapsPreservingIdentity}
Niech $S: M_{2} \rightarrow M_{2}$ będzie dodatnim odwzorowaniem zachowującym
macierz jednostkową. Rozważmy następujące warunki:
\begin{enumerate}

\item
\label{lem:condProj}
$S$ jest odwzorowaniem Schwarza oraz istnieje projektor ortogonalny $P$, rzędu 1, dla którego $S(P)^{2} = S(P)$.

\item
\label{lem:condCommut}
Istnieje projektor ortogonalny P rzędu 1, taki że dla każdego $X \in M_{2}$,
$S([P,X]) = [S(P), \, S(X)]$.

\item
\label{lem:condCP}
$S$ jest kompletnie dodatnie.
\end{enumerate}
Wówczas pomiędzy warunkami powyżej zachodzi relacja:
$
\ref{lem:condProj} \Rightarrow
    \ref{lem:condCommut} \Rightarrow \ref{lem:condCP}.
$
\end{Theorem}

\begin{proof}
$\ref{lem:condProj}) \Rightarrow \ref{lem:condCommut})$.

Niech $A_{k} = kP + iX$, dla pewnego $X = X^{*} \in M_2$
oraz $k \in \mathbb{N}$.
Kładąc $A_{k}$ w nierówności Kadisona-Schwarza
\eqref{eq:SchwarzInequality}, otrzymujemy
\begin{linenomath*}
 \begin{equation}
i [S(P), \,  S(X)] - i S([P, \, X])  \leq \frac{1}{k} \left( S(X^{2}) - S(X)^{2} \right),
 \end{equation}
\end{linenomath*}
dla każdego $k \in \mathbb{N}$.
Zatem
\begin{linenomath*}
 \begin{equation}
\label{ineq:Commutators}
i [S(P), \, S(X)] - i S([P, \, X]) \leq 0.
 \end{equation}
\end{linenomath*}
Biorąc tym razem $A'_{k} =kP - i X$ i powtarzając rachunek,
dostajemy nierówność odwrotną do \eqref{ineq:Commutators},
a stąd
\begin{linenomath*}
 \begin{equation}
\label{eq:Commutators}
S([P, X]) = [S(P), S(X)],
 \end{equation}
\end{linenomath*}
dla każdej hermitowskiej macierzy $X$.
Ponieważ równanie \eqref{eq:Commutators} jest liniowe w $X$,
zachodzi również dla każdej macierzy $X \in M_{2}$.

$\ref{lem:condCommut}) \Rightarrow \ref{lem:condCP})$.
Niech $P$ będzie projektorem ortogonalnym rzędu 1, takim że
\begin{linenomath*}
 \begin{equation}
\label{eq:Commutator}
S([P,X]) = [S(P), \, S(X)],
 \end{equation}
\end{linenomath*}
dla każdego $X \in M_{2}$.
Jeśli $S$ jest kompletnie dodatnim odwzorowaniem, wówczas dla każdej pary
unitarnych macierzy $U, V \in M_{2}$,
odwzorowanie $\tilde{S}: A \mapsto U^{*} S(V^{*} A V) U$
jest również kompletnie dodatnie.
Możemy zatem założyć bez straty ogólności, że
$P = P_{1} =
\left(
\begin{smallmatrix} 1 & 0 \\ 0 & 0 \end{smallmatrix}
\right)$,
i $S(P)$ jest diagonalne.
Niech $P_{2} = \left(
\begin{smallmatrix} 0 & 0 \\ 0 & 1 \end{smallmatrix}
\right)$,
$E_{12} =
\left(
\begin{smallmatrix} 0 & 1 \\ 0 & 0 \end{smallmatrix}
\right)$,
and
$E_{21} =
\left(
\begin{smallmatrix} 0 & 0 \\ 1 & 0 \end{smallmatrix}
\right)$.
Załóżmy, że
$S(P_{1}) = \lambda_{1} P_{1} + \lambda_{2} P_{2}$,
$0 \leq \lambda_{2} \leq \lambda_{1} \leq 1$.
Jeśli $S(E_{12}) = 0$,
wówczas oczywiście również $S(E_{21}) = 0$ i w rezultacie
$S$ odwzorowuje $M_{2}$
w przemienną algebrę macierzy diagonalnych.
Stąd $S$ jest kompletnie dodatnie.
Z drugiej strony, jeśli
$S(E_{12}) =
\left(
\begin{smallmatrix} w & z \\ x & y \end{smallmatrix}
\right) \neq 0$,
$w, z, x, y \in \mathbb{C}$
i gdy położymy $X = E_{12}$ we wzorze \eqref{eq:Commutator},
pamiętając jednocześnie, że $\lambda_{1} \geq \lambda_{2}$,
otrzymujemy $x = y = w = 0$ i $\lambda_{1} - \lambda_{2} = 1$.
To oznacza, że $\lambda_{1} = 1$ i $\lambda_{2} = 0$ oraz
$S(E_{12}) = z E_{12}$.
A zatem $S$ działa na macierz
$X \in M_{2}$, $X = (x_{ij})_{i,j=1,2}$ jak
\begin{linenomath*}
 \begin{equation}
S(X) = \begin{pmatrix}
 x_{11} & z \, x_{12} \\
\overline{z} \, x_{21} & x_{22}
\end{pmatrix}.
 \end{equation}
\end{linenomath*}
Stąd wynika już w prosty sposób, że $S$ jest kompletnie dodatnie.
Rzeczywiście,
z dodatniości $S$ mamy $|z| \leq 1$.
Jeśli napiszemy
\begin{linenomath*}
 \begin{equation}
S(X) =
\begin{pmatrix}
1 & z \\ \overline{z} & 1
\end{pmatrix} \circ X,
 \end{equation}
\end{linenomath*}
gdzie symbol $\circ$ w tym przypadku oznacza iloczyn Hadamarda macierzy,
tj. iloczyn, w którym mnoży się ze sobą macierze tego samego wymiaru
element po elemencie.
Ponieważ $|z| \leq 1$, macierz
$
\left(
\begin{smallmatrix} 1 & z \\ \overline{z} & 1 \end{smallmatrix}
\right)$
jest dodatnia.
Ogólnie znany jest fakt, że odwzorowanie $X \mapsto A \circ X$
jest kompletnie dodatnie wtedy i tylko wtedy, gdy $A$
jest dodatnią macierzą
(por. np. lemat 1 \cite{besenyei2011completely}).
\end{proof}

\begin{Example}
Można bez trudu pokazać, że warunek \ref{lem:condProj}. w
twierdzeniu \ref{thm:MapsPreservingIdentity} jest w istotny sposób silniejszy
niż warunek \ref{lem:condCommut}.
Niech $S$ będzie odwzorowaniem bistochastycznym na $M_{2}$, takim że
\begin{linenomath*}
 \begin{equation}
\label{RandomLabel:514431}
    \pi(S) = \begin{pmatrix}
    1 & 0 & 0 & 0 \\
    0 & b & 0 & 0 \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0
    \end{pmatrix},
    \quad 0 < b < 1,
 \end{equation}
\end{linenomath*}
czyli $S(\sigma_{1}) = b \sigma_{1}$, a także
$S(\sigma_{2}) = S(\sigma_{3}) = 0$.
Wówczas $S\left( [P_{0}, \, X ] \right) = [ S(P_{0}), \, S(X) ]$,
gdzie
$P_{0}= \frac{1}{2} \left( \begin{smallmatrix}
 1 & 1 \\ 1 & 1
 \end{smallmatrix} \right)$ i $X \in M_{2}$.
Jednakże nie istnieje projektor ortogonalny $P$ rzędu 1,
dla którego $S(P)^{2} = S(P)$.
Rzeczywiście, skoro
$P_{0} = \frac{1}{\sqrt{2}}(\sigma_{0} + \sigma_{1})$,
$S(P_{0}) = \frac{1}{\sqrt{2}}(\sigma_{0} + b \sigma_{1})$.
Niech $X$ będzie macierzą hermitowską, $X = \sigma(a)$, $x \in \mathbb{R}^{4}$,
$S(X) = a_{0} \sigma_{0} + b a_{1} \sigma_{1}$.
Wtedy $[S(P_{0}), \, S(X) ] =0$.
Co więcej,
$[P_{0}, \, \sigma(a)] =
i(a_{2} \sigma_{3} - a_{3} \sigma_{2})$,
a stąd
$S([P_{0}, \, \sigma(a)]) = 0$.
Warunek \ref{lem:condCommut}. w Twierdzeniu \ref{thm:MapsPreservingIdentity}
jest prawdziwy dla każdej macierzy hermitowskiej $X$,
a zatem również dla każdej macierzy $X \in M_{2}$.
Z drugiej strony,
$P = \sigma(a)$, $a \in \mathbb{R}^{4}$, jest projektorem ortogonalnym
rzędu 1,
wtedy i tylko wtedy, gdy $a_{0} = ||\vec{a}||_{2} = \frac{1}{\sqrt{2}}$.
Zatem $S(P) \neq \mathbf{1}$, a ponieważ $b<1$,
więc $S(P)$ nie może być projektorem ortogonalnym \mbox{rzędu 1},
tzn. $S(P)^{2} \neq S(P)$.
\end{Example}

Dla odwzorowań ekstremalnych warunek \ref{lem:condProj} z
twierdzenia \ref{thm:MapsPreservingIdentity} okazuje się również wystarczający
do tego, aby takie odwzorowanie było kompletnie dodatnie dla każdego wymiaru.
Bardziej ogólny problem, postawiony przez Robertsona
\cite{robertson1983schwarz},
czy istnieje odwzorowanie Schwarza, które jest ekstremalne, ale
nie 2-dodatnie, pozostaje otwarty.
Zauważmy, że z twierdzenia 3.3 w pracy
\cite{marciniak2008extremal}, wynika, iż
każde ekstremalne odwzorowanie 2-dodatnie jest kompletnie dodatnie.

\begin{Theorem}
\label{thm:ExtremalSchwarz}
Niech $S: M_{n} \rightarrow M_{n}$
będzie odwzorowaniem Schwarza, ekstremalnym w zbiorze wszystkich odwzorowań
dodatnich. Załóżmy dodatkowo, że
istnieje para projektorów ortogonalnych $P, Q \in M_{n}$, rzędu 1,
dla których $S(P) = Q$.
Wówczas $S$ jest kompletnie dodatnie.
\end{Theorem}
\begin{proof}
Niech $(e_{i})_{i=1}^{n}$
będzie standardową bazą ortonormalną w $\mathbb{C}^{n}$.
Ponieważ chcemy pokazać, że $S$ jest kompletnie dodatnie,
możemy założyć, że $P = Q = P_{n}$,
gdzie $P_{n} = e_{n} e_{n}^{*}$ jest projektorem ortogonalnym na jednowymiarową
przestrzeń rozpiętą przez wektor $e_{n} = (0,0,\ldots,0,1) \in \mathbb{C}^{n}$.
Z faktu, że $S$ zachowuje macierz jednostkową, wynika
$S(P_{n}^{\perp}) = P_{n}^{\perp}$,
gdzie $P_{n}^{\perp} = \mathbf{1}_{n} - P_{n}$.
Dla macierzy $X \in M_{n}$,
zapisanej w postaci blokowej
$X = \left( \begin{smallmatrix} A & u \\ w^{t} & z \end{smallmatrix} \right)$,
gdzie $A \in M_{n-1}$, $u, w \in \mathbb{C}^{n-1}$ są kolumnami, a
$z \in \mathbb{C}$, mamy
\begin{linenomath*}
 \begin{equation}
\label{eq:SBlockForm}
    S (X) = \begin{pmatrix}
        \hat{S}_{0}(A) & S_{1} u \\
        (\overline{S}_{1} w)^{t} & z
    \end{pmatrix},
 \end{equation}
\end{linenomath*}
dla $\hat{S}_{0}: M_{n-1} \rightarrow M_{n-1}$ będącego z konieczności
odwzorowaniem Schwarza na $M_{n-1}$ i $S_{1} \in M_{n-1}$.
Rzeczywiście, ponieważ $S$ jest odwzorowaniem Schwarza,
na mocy twierdzenia 2.1.5 \cite{Stormer2013},
$S(P_{n} X) = P_{n} S(X)$.
Stąd
$S(P_{n} X P_{n}) = P_{n} S(X) P_{n}$ i podobnie
$S(P_{n}^{\perp} X P_{n}^{\perp}) = P_{n}^{\perp} S(X) P_{n}^{\perp}$,
$S(P_{n}^{\perp} X P_{n}) = P_{n}^{\perp} S(X) P_{n}$,
$S(P_{n} X P_{n}^{\perp}) = P_{n} S(X) P_{n}^{\perp}$,
co dowodzi, że $S$ musi mieć postać jak w równaniu \eqref{eq:SBlockForm}.

Jeśli weźmiemy jednowymiarowy projektor ortogonalny
$P_{u} = u u^{*}$, gdzie
$u = (u_{1}, u_{2}, \ldots, u_{n}) = (\vec{u}, u_{n})$,
$\vec{u} = (u_{1}, u_{2}, \ldots, u_{n-1}) \in \mathbb{C}^{n-1}$,
$u_{n} \in \mathbb{C}$,
dostaniemy
\begin{linenomath*}
 \begin{equation}
 SP_{u} \:=\:  S \begin{pmatrix}
    \vec{u} \vec{u}^{\,*} & \overline{u}_{n} \vec{u} \\
    u_{n} \vec{u}^{\,*}   & |u_{n}|^{2}
 \end{pmatrix} \: = \:
 \begin{pmatrix}
    \hat{S}_{0}(\vec{u} \vec{u}^{*}) &
         \overline{u}_{n} S_{1} \vec{u} \\
    u_{n} ( S_{1} \vec{u} )^{*} &
        |u_{n}|^{2}
 \end{pmatrix}.
 \end{equation}
\end{linenomath*}
W przypadku, gdy $u_{n} \neq 0$,
biorąc dopełnienie Schura
(por. F.\,Zhang, \emph{The Schur complement and its applications} \cite{Zhang2006}),
mamy, że $SP_{u} \geq 0$, wtedy i tylko wtedy, gdy
\begin{linenomath*}
 \begin{equation}
\label{ieq:SchurForS}
  S_{1} \vec{u} \vec{u}^{*} S_{1}^{*} \leq \hat{S}_{0}(\vec{u}\vec{u}^{*}).
 \end{equation}
\end{linenomath*}
To oznacza, że $S$ jest sumą dwóch dodatnich odwzorowań działających na
jednowymiarowy projektor ortogonalny jak poniżej:
\begin{linenomath*}
 \begin{equation}
   \label{eq:decompose2e34}
 SP_{u} \:=\:
 \begin{pmatrix}
      S_{1} \vec{u} \vec{u}^{*} S_{1}^{*}  &
         \overline{u}_{n} S_{1} \vec{u} \\
    u_{n} ( S_{1} \vec{u} )^{*} &
        |u_{n}|^{2}
 \end{pmatrix} +
 \begin{pmatrix}
    \hat{S}_{0}(\vec{u} \vec{u}^{*})  -  S_{1} \vec{u} \vec{u}^{*} S_{1}^{*} & 0 \\
    0 & 0
 \end{pmatrix}.
 \end{equation}
\end{linenomath*}
Skoro $S$ jest z założenia ekstremalne i  $S(P_{n}) = P_{n}$,
więc drugi wyraz w \eqref{eq:decompose2e34} znika i otrzymujemy:
\begin{linenomath*}
 \begin{equation}
 SP_{u} \:=\:
 \begin{pmatrix}
      S_{1} \vec{u} \vec{u}^{*} S_{1}^{*}  &
         \overline{u}_{n} S_{1} \vec{u} \\
    u_{n} ( S_{1} \vec{u} )^{*} &
        |u_{n}|^{2}
 \end{pmatrix} =
    \begin{pmatrix}
    S_{1} & 0 \\ 0 & 1
    \end{pmatrix}
    \, P_{u} \,
    \begin{pmatrix}
    S_{1} & 0 \\ 0 & 1
    \end{pmatrix}^{*}.
 \end{equation}
\end{linenomath*}
Jeśli położymy $U = \left( \begin{smallmatrix} S_{1} & 0 \\ 0 & 1
\end{smallmatrix} \right) \in M_{n}$,
z faktu, że $S(P_{n}^{\perp}) = P_{n}^{\perp}$,
dostaniemy $U U^{*} = \mathbf{1}_{n}$, tj. $U$ macierzą unitarną oraz
$S(X) = U X U^{*}$, $X \in M_{n}$, co kończy dowód.
\end{proof}

Przedstawiwszy geometryczny dowód twierdzenia \ref{thm:PositiveMapsOnM2}
o rozkładzie odwzorowań dodatnich na $M_{2}$, możemy pokusić się o
o udowodnienie dodatkowych faktów dotyczących odwzorowań bistochastycznych.
Odznaczmy przez $\Delta_{\infty}$ zbiór wszystkich bistochastycznych odwzorowań na
$M_{2}$.
Niech także $\Delta_{1} \subset \Delta_{\infty}$ oznacza zbiór tych odwzorowań
bistochastycznych,
które są jednoczenie kompletnie dodatnie i kompletnie kododatnie.
Korzystając z tego samego geometrycznego przedstawienia zbioru
odwzorowań dodatnich jako stożka Lorentza $L_{4}$,
możemy pokazać, że $\Delta_{1}$ jest zbiorem zaskakująco pojemnym.
Dodajmy, że wyniki odnośnie tego problemu można znaleźć również w innych
publikacjach \cite{stormer2013decomposition}.

Każda macierz $Y \in M_{n}(\mathbb{R})$ daje się rozłożyć względem wartości singularnych:
$Y = O_{1} D O_{2}$, gdzie
$O_{1}, O_{2}$ to macierze ortogonalne tego samego wymiaru, co $Y$, a
$D = \text{diag}(t_{1}, t_{2}, \ldots, t_{n})$ jest macierzą diagonalną,
taką że wartości singularne $Y$ dają się uporządkować:
$t_{1} \geq t_{2} \geq \ldots t_{n} \geq 0$.
Innymi słowy, wartości singularne $Y$ to wartości własne macierzy
$|Y| = (Y^{t} Y)^{1/2}$.
W przestrzeni operatorów $M_{n}(\mathbb{R})$ możemy wyróżnić wiele norm;
jedną z nich jest norma operatorowa: $|| \cdot ||$;
drugą nazwiemy normą śladową i zdefiniujemy przez
$||Y||_{1} = \sqrt{\text{Tr} \, Y^{t} \, Y }$.
Domkniętą kulę o jednostkowym promieniu w normie operatorowej oznaczmy:
$K_{\infty} = \left \{ Y \in M_{3}(\mathbb{R}): \, ||Y|| \leq 1 \right \}$;
natomiast kulę w normie śladowej jako
$K_{1} = \left \{ Y \in M_{3}(\mathbb{R}): \, ||Y||_{1} \leq 1 \right \}$.

\begin{Theorem}
\label{thm:Ball}
Istnieje wypukły izomorfizm $F$, taki że $K_{\infty}$, $F(\Delta_{\infty}) = K_{\infty}$.
Ponadto, $F(\Delta_{1}) = K_{1}$.
\end{Theorem}
\begin{proof}
Niech $S$ będzie odwzorowaniem bistochastycznym na $M_{2}$.
Macierz $\pi(S)$ możemy zapisać w postaci blokowej
\begin{linenomath*}
 \begin{equation}
\label{eq:Sbistochastic}
\pi(S) = \begin{pmatrix}
    1  &  \vec{0}^{t} \\
    \vec{0} & Y
    \end{pmatrix},
 \end{equation}
\end{linenomath*}
$Y \in M_{3}(\mathbb{R})$,
a $\vec{0} \in \mathbb{R}^{3}$ jest zerowym wektorem-kolumną.
Połóżmy $F(S) = Y$.
Z definicji \eqref{def:PiofSiso}
odwzorowania $\pi$,
jasne jest, że dla  $S_{1}, S_{2} \in \Delta_{\infty}$,
mamy $F \left( \lambda S_{1} + (1-\lambda) S_{2} \right) =
 \lambda F(S_{1}) + (1-\lambda) F(S_{2})$,
$0 \leq \lambda \leq 1$
oraz $F(S_{1} S_{2}) = F(S_{1}) F(S_{2})$.
Odwzorowanie $S$ jest dodatnie wtedy i tylko wtedy, gdy
$\pi(S)$ zachowuje stożek $L_{4}$,
co w tym wypadku jest równoważne temu, iż $F(S) = Y \in K_{\infty}$.
Co więcej, łatwo zauważyć, że oba przekształcenia, $F$ i $F^{-1}$,
są ciągłe w topologii indukowanej przez normy na
$\Delta_{\infty}$ i $K_{\infty}$.
Zatem $F(\Delta_{\infty}) = K_{\infty}$, a ponadto $F$ jest homeomorfizmem.

W dalszej części załóżmy, że $S \in \Delta_{1}$.
Ponieważ $S$ jest kompletnie dodatnie,
z twierdzenia 1 w pracy L.\,J.\,Landaua i R.\,F.\,Streatera
\cite{landau1993birkhoff} wiemy, że
$S$ jest wypukłą kombinacją odwzorowań unitarnych:
$X \mapsto U^{*} X U$, $U \in \text{U}(2)$.
Z dowodu twierdzenia \ref{thm:PositiveMapsOnM2},
dla konkretnej postaci $\pi(S)$, takiej jak w równaniu \eqref{eq:Sbistochastic},
oznacza to, że $Y$ należy do wypukłej otoczki generowanej przez elementy grupy $\text{SO}(3)$.
Z  wniosku na str. 139 w pracy H.\,F.\,Miranda i R.\,C.\,Thompson
\cite{miranda1994group},
dostajemy, że jeśli $t_{1} \geq t_{2} \geq t_{3} \geq 0$
są wartościami singularnymi macierzy $Y$,
wówczas $Y \in \text{conv} \, \text{SO}(3)$,
wtedy i tylko wtedy, gdy
\begin{linenomath*}
 \begin{equation}
\label{eq:ineqForSingVals}
t_{1} \leq 1
\quad \text{oraz    } \quad
t_{1} + t_{2} - \text{sig} (\text{det} Y) \, t_{3} \leq 1.
 \end{equation}
\end{linenomath*}
Skoro $S$ jest również odwzorowaniem kompletnie kododatnim, tzn.
$S' = S \circ t$ jest kompletnie dodatnim bistochastycznym odwzorowaniem.
Niech $F(S') = Y'$.
Wówczas $Y' = Y \, \text{diag} (1,-1,1)$.
Obie macierze, $Y$ i $Y'$, mają te same wartości singularne oraz
$\text{det} \, Y' = - \text{det} \, Y$.
Ponieważ $Y' \in  \text{conv} \, \text{SO}(3)$, mamy
$t_{1} + t_{2} + \text{sig} (\text{det} Y) \, t_{3} \leq 1$.
Łącząc ten fakt z równaniem \eqref{eq:ineqForSingVals},
skoro $\text{det} \, Y = 0$, wtedy i tylko wtedy, gdy  $t_{3} = 0$,
otrzymujemy, że $S$ jest jednocześnie kompletnie dodatnie i kompletnie kododatnie,
wtedy i tylko wtedy, gdy
$||Y||_{1} = t_{1} + t_{2} + t_{3} \leq 1$,
co w rezultacie oznacza, że $F(\Delta_{1}) = K_{1}$
i co kończy dowód.
\end{proof}

Podsumowując, powyżej przedstawiliśmy geometryczne podejście do problemu
analizy odwzorowań dodatnich, przynajmniej w najprostszym przypadku odwzorowań
algebry macierzy $M_{2}$.
W szczególności,
pokazaliśmy, że dzięki tym metodom można w zwarty i elegancki sposób dowieść
znanego twierdzenia o rozkładzie odwzorowań dodatnich na sumę przekształceń
kompletnie dodatnich i kompletnie kododatnich,
które okazuję się być bezpośrednio zależne od faktu, że każdy operator
zachowujący stożek Lorentza w $\mathbb{R}^{n}$ jest wypukłą kombinacją tych
operatorów, które dodatkowo zachowują brzeg stożka.
Jak już wspomnieliśmy, twierdzenie dowiedzione powyżej
przekłada się na kryterium separowalności stanów kwantowych dla układu
złożonego dwóch qubitów i otrzymuję formę znanego kryterium PPT
\cite{peres1996separability,horodecki1996separability}.
Dowód kolejnego faktu, twierdzenia \ref{thm:MapsPreservingIdentity},
chociaż nie posługuje się tymi samymi metodami geometrycznymi w sposób jawny,
bazuje na wiedzy dotyczącej operatorów zachowujących stożek Lorentza i posiadających
dodatkowo normę równą jedności.
Stąd pochodzi założenie \ref{lem:condProj}. w twierdzeniu \ref{thm:MapsPreservingIdentity}.
Wreszcie, twierdzenie \ref{thm:Ball} pokazuje, że zbiór odwzorowań bistochastycznych,
które są jednocześnie kompletnie dodatnie i kompletnie kododatnie jest
topologicznie tożsamy z kulą jednostkową w $M_{3}(\mathbb{R})$ w normie śladowej.


\section{Stabilne podalgebry jako metoda analizy własności odwzorowań na $M_{3}$}
\label{sec:M3notes}

W następnym podrozdziale skupimy się na próbie pokazania,
jak geometryczne metody analizy odwzorowań dodatnich,
ukazane powyżej,
dają się uogólnić na kolejny nietrywialny przypadek odwzorowań algebry $M_{3}$.
W tej części zajmijmy się zbudowaniem niezbędnego aparatu matematycznego,
opartego na istnieniu stabilnych podprzestrzeni odwzorowań bistochastycznych
i ich klasyfikacji pod kątem struktury algebr Jordana.
To pozwoli nam na wskazanie interesującego przykładu ekstremalnych odwzorowań
dodatnich, który później postaramy się zaklasyfikować, mając już do dyspozycji
geometryczne metody z następnego podrozdziału.

Przypomnijmy tylko, że przez JB$^{*}$-algebrę $K$ rozumiemy zespoloną
przestrzeń Banacha, która jest jednocześnie algebrą Jordana.
W rozpatrywanym przez nas przypadku przestrzeni,
których elementami są zespolone macierze kwadratowe,
oznacza to, że dla każdego $A, B \in K$, mamy
$AB + BA \in K$.
Będziemy zawsze zakładać, że $\mathbf{1} \in K$.
Dokładne przedstawienie teorii JB$^{*}$-algebr można odnaleźć w książce
H.\,Hanche-Olsen, E.\,St{\o}rmer,
\mbox{\emph{Jordan operator algebras} \cite{Hanche1984}}.

\vspace{0.5cm}
Niech $S\!: M_{n} \rightarrow M_{n}$ będzie odwzorowaniem bistochastycznym.
Łatwo zauważyć, że $S$ jest kontrakcją w normie Hilberta-Schmidta
na $M_{n}$, zdefiniowanej jako
$||A||_{HS} = \left( \text{Tr} \, A^{*} A \right)^{1/2}$.
Rzeczywiście, skoro $S$ spełnia nierówność Kadisona-Schwarza
\eqref{eq:SchwarzInequality}
dla każdej normalnej macierzy $A \in M_{n}$,
zakładając najpierw, że $A = A^{*}$,
mamy
\begin{linenomath*}
 \begin{equation}
\label{RandomLabel:712874}
    || S(A) ||_{HS}^{2} \: = \: \text{Tr} S(A)^{2}
    \: \leq \:  \text{Tr} S(A^{2}) \: = \:
        \text{Tr} A^{2} \: = \: ||A||_{HS}^{2}.
 \end{equation}
\end{linenomath*}
Ogólną macierz $A \in M_{n}$ można przedstawić jako sumę
$A = A_{1} + i A_{2}$,
gdzie oba składniki, $A_{1}$ i $A_{2}$, to macierze hermitowskie.
Powtarzając w zasadzie ten sam rachunek, co w równaniu
\eqref{RandomLabel:712874},
dostajemy, że $S$ jest kontrakcją w normie Hilberta-Schmidta.

Zdefiniujmy izometryczne rozbicie (ang. \emph{isometric splitting})
odwzorowania $S$, korzystając z wyników opublikowanych wcześniej w pracy
\cite{olkiewicz1999environment}.
Ponieważ $S$ jest kontrakcją na przestrzeni Hilberta $M_{n}$
z iloczynem tensorowym Hilberta-Schmidta,
przestrzeń tę można rozłożyć na sumę prostą przestrzeni $K_{S}$,
zdefiniowanej jako
\begin{linenomath*}
 \begin{equation}
\label{def:definitionofK}
    K_{S} \: = \: \left\{
        A \in M_{n}: \,\,
            || S^{k} A ||_{HS} = || S^{* k} A ||_{HS} = || A ||_{HS}, \:\:
            \forall k \in \mathbb{N}
        \right\}
 \end{equation}
\end{linenomath*}
oraz jej ortogonalnego dopełnienia $K_{S}^{\perp}$.
Oznaczmy przez $S^{*}$ odwzorowanie bistochastyczne na $M_{n}$,
sprzężone do $S$ jako operator liniowy na przestrzeni Hilberta $M_{n}$,
tj. $\text{Tr}\, S^{*} (A)\, B = \text{Tr} A \, S(B)$,
dla każdego $A, B \in M_{n}$.
Prawdą jest, że $A \in K_{S}$,
wtedy i tylko wtedy, gdy
$S^{* k} S^{k} A = S^{k} S^{* k} A = A$,
dla każdego $k \in \mathbb{N}$.
Następny lemat, przepisany z niewielkimi zmianami z
powyżej cytowanej pracy,
zbiera w jednym miejscu najważniejsze własności przestrzeni $K_{S}$.

\begin{Lemma}
\label{lem:propertiesofK}
Niech $S$ będzie odwzorowaniem bistochastycznym na $M_{n}$
oraz niech $K_{S}$ będzie zdefiniowane jak w równaniu
\eqref{def:definitionofK}.
Wówczas:
% %Enumerate the list like: a), b), c) etc.
\let \oldlabelenumi \labelenumi
\renewcommand{\labelenumi}{\alph{enumi})}
\begin{enumerate}
\item $\mathbf{1} \in K_{S}$;
\item $A \in K_{S}$ implikuje $A^{*} \in K_{S}$;
\item  $A  \in K_{S}$ implikuje
$|A| = (A^{*}A)^{1/2} \in K_{S}$;
\item jeśli $A, B \in K_{S}$, to $AB + BA \in K_{S}$;
\item jeśli $A = A^{*} \in K_{S}$ oraz $A = \sum_{i=1}^{k} \lambda_{i} P_{i}$,
gdzie każde $\lambda_{i} \neq 0$ jest różne i
każde $P_{i}$ jest projektorem ortogonalnym w $M_{n}$,
to $P_{i} \in K_{S}$ dla każdego $i = 1,2,\ldots,k$;
\item jeśli $P \in K_{S}$ jest projektorem ortogonalnym,
to $S(P)$ i $S^{*}(P)$ są również projektorami ortogonalnymi i
$\mathrm{dim} \, S(P) = \mathrm{dim} \, S^{*}(P) = \mathrm{dim} \, P$;
\item jeśli $P, Q \in K_{S}$ są projektorami ortogonalnymi, dla których
$P Q = 0$,
to $S(P) S(Q) = S^{*}(P) S^{*}(Q) = 0$.
\end{enumerate}
%Return to the old enumaration style
\let \labelenumi \oldlabelenumi
\end{Lemma}

\begin{proof}
a) Oczywiste, ponieważ $S$ jest odwzorowaniem bistochastycznym;
b), c) dowód tak jak w stwierdzeniu 5\,a) i 5\,b) w \cite{olkiewicz1999environment}.

d)  Załóżmy na początku, że $A = A^{*} \in K_{S}$.
Skoro $S^{*k} S^{k}(A) = S^{k} S^{*k}(A) = A$,
a także na mocy nierówności Kadisona-Schwarza zastosowanej do odwzorowania
$S^{*k} S^{k}$, mamy
\begin{linenomath*}
 \begin{equation}
A^{2} \: = \: \left( S^{*k} S^{k}(A) \right) \left( S^{*k} S^{k}(A) \right)
    \: \leq \: S^{*k} S^{k}(A^{2}).
 \end{equation}
\end{linenomath*}
Stąd
\begin{linenomath*}
 \begin{equation}
||A^{2}||_{HS} \: \leq \: || S^{*k} S^{k}(A^{2}) ||_{HS} \: \leq \:
    || S^{k}(A^{2}) ||_{HS} \: \leq \: ||A^{2}||_{HS},
 \end{equation}
\end{linenomath*}
tzn. $|| S^{k}(A^{2}) ||_{HS} = ||A^{2}||_{HS}$.
Podobny argument dowodzi, że
$|| S^{*k}(A^{2}) ||_{HS} = ||A^{2}||_{HS}$,
i dlatego $A^{2} \in K_{S}$.
Dla każdego $A, B \in K_{S}$, takich że $A = A^{*}$, $B = B^{*}$,
mamy
$AB + BA = (A + B)^{2} - A^{2} - B^{2} \in K_{S}$.
W ogólności, dla $A \in K_{S}$, piszemy
$A = A_{1} + i A_{2}$, gdzie $A_{1}, A_{2}$ są hermitowskie.
Wtedy
$A^{2} = A_{1}^{2} - A_{2}^{2} + i(A_{1} A_{2} + A_{2} A_{1}) \in K_{S}$.
Jasne jest teraz, że $AB+BA \in K_{S}$ dla ogólnych $A,B \in K_{S}$.

e) tak jak stw. 5\,d);
f), g) tak jak stw. 6\,a) i 6\,c) w
\cite{olkiewicz1999environment}.
\end{proof}

Następujący fakt jest prostą konsekwencją poprzedniego.

\begin{Corollary}
\label{cor:KisJordanAlgebra}
Przestrzeń $K_{S}$,
z normą operatorową $|| \cdot ||$
i mnożeniem $A \circ B = \frac{1}{2}(AB + BA)$,
jest JB$^{*}$-algebrą.
Odwzorowanie $S$ jest automorfizmem Jordana na $K_{S}$ oraz
\begin{linenomath*}
 \begin{equation}
\label{eq:SGoesTo0OnKOrth}
    \lim \limits_{k\rightarrow \infty} S^{k}(A)  \: = \:
    \lim \limits_{k\rightarrow \infty} S^{*k}(A) \: = \: 0,
 \end{equation}
\end{linenomath*}
dla każdego $A \in K_{S}^{\perp}$.
\end{Corollary}

\begin{proof}
Jest oczywiste, że $K_{S}$ jest zespoloną algebrą Banacha.
Z lematu \ref{lem:propertiesofK}\,b),
$K_{S}$ posiada inwolucję,
a z punktu d) wiemy, że $K_{S}$ jest algebrą Jordana.
Ponieważ odwzorowanie $S$ jest niezmiennicze względem podprzestrzeni
$K_{S}$ i $K_{S}^{\perp}$,
i rzecz jasna $M_{n} = K_{S} \oplus K_{S}^{\perp}$,
wnioskujemy, że $S$ rozbija się na sumę prostą
$S = S_{1} \oplus S_{2}$,
gdzie $S_{1} = S_{| K_{S}}$, $S_{2} = S_{| K_{S}^{\perp}}$.
Na mocy stw. 7a) w \cite{olkiewicz1999environment},
mamy, że $S(A^{*} A) = S(A)^{*} S(A)$, dla $A \in K_{S}$,
tzn. $S$ jest homomorfizmem Jordana na $K_{S}$.
Co więcej, ponieważ $S^{*} S = S S^{*} = I$ na $K_{S}$,
odwzorowanie $S_{1}$ jest odwracalne,
a zatem jest automorfizmem Jordana
(por. definicję 3.2.1(6) w \cite{Bratteli2003}).
Z definicji \eqref{def:definitionofK} przestrzeni $K_{S}$,
wynika w łatwy sposób, że
równanie \eqref{eq:SGoesTo0OnKOrth} jest spełnione.
\end{proof}

\begin{Theorem}
\label{thm:FromESbook}
Niech $K \subset M_{n}$ będzie JB$^{*}$-podalgebrą $M_{n}$.
Wówczas istnieje odwzorowanie bistochastyczne
$S \! : M_{n} \rightarrow M_{n}$, takie że $K = K_{S}$.
\end{Theorem}
\begin{proof}
Popatrzmy na przestrzeń $M_{n}$ jak na przestrzeń Hilberta z iloczynem skalarnym
Hilberta-Schmidta.
Niech $S\!: M_{n} \rightarrow K \subset M_{n}$ będzie
projektorem ortogonalnym na $K$.
Jest jasne, że $S(\mathbf{1}) = \mathbf{1}$,
a ponieważ $S = S^{*}$,
odwzorowanie $S$ zachowuje również ślad oraz
$K$ jest stabilną podprzestrzenią dla $S$,
zakładając, że $S$ jest odwzorowaniem dodatnim.
Niech $A \in M_{n}$  będzie dodatnią macierzą.
Ponieważ $K$ jest JB$^{*}$-algebrą,
$(SA)^{*} \in K$.
Z faktu, że
$|| A - SA ||_{HS} = || A - (SA)^{*} ||_{HS}$
oraz $SA$ jest najlepszym przybliżeniem $A$ w przestrzeni $K$,
mamy $(SA)^{*} = SA$.
Zapiszmy $SA = B_{+} - B_{-}$,
gdzie obie macierze $B_{+}, B_{-}$ są dodatnie i
$B_{+} B_{-} = 0$.
Z założenia $\mathbf{1} \in K$,
a ponieważ $(SA)^{k} \in K$ dla każdego $k \in \mathbb{N}$,
wiemy, że również moduł $|S A| \in K$
(por. jawny wzór na pierwiastek kwadratowy z dodatniej macierzy
\cite{Bratteli2003}, s. 34).
Zatem oba $B_{+}, B_{-} \in K$.
Obliczmy
\begin{linenomath*}
 \begin{multline}
|| A - SA ||_{HS}^{2} \: = \: \text{Tr} \, ( A - SA )^{2} \: = \:
    \text{Tr} \, ( A - B_{+} + B_{-} )^{2} \: = \: \\
    \text{Tr} \, ( A - B_{+} )^{2} +
        2 \, \text{Tr} \, ( A - B_{+} ) B_{-} + \text{Tr} \, B_{-}^{2}
            \: = \: \\
    || A - B_{+} ||_{HS}^{2} +
        2 \, \text{Tr} \,  A \, B_{-}+ \text{Tr} \, B_{-}^{2}
    \: \geq \: || A - B_{+} ||_{HS}^{2}.
 \end{multline}
\end{linenomath*}
Ponieważ $SA$ jest najlepszym przybliżeniem $A$ in $K$,
dostajemy, że $SA = B_{+}$, tzn. $SA \geq 0$, co kończy dowód.
\end{proof}


Z powyższego wynika, że $K_{S}$ posiada strukturę JB$^{*}$-algebry.
Jak zobaczymy w dalszej części,
z dodatkowym założeniem odnośnie odwzorowania S,
taka algebra musi być bardzo szczególna,
przynajmniej w nisko wymiarowym przypadku.

Dla wygody oznaczmy ortogonalne projektory:
\begin{linenomath*}
 \begin{equation}
\label{def:OrthogonalProjections}
    P_{1} = \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 0 & 0 \\
        0 & 0 & 0
    \end{pmatrix} , \quad \quad
    P_{2} = \begin{pmatrix}
        0 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 0
    \end{pmatrix} , \quad \quad
    P_{3} = \begin{pmatrix}
        0 & 0 & 0 \\
        0 & 0 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
 \end{equation}
\end{linenomath*}
oraz $P_{12} = P_{1} + P_{2}$.

\label{page:allpossibleJalg}
Ważny dla dalszego toku rozumowania jest wniosek z twierdzeń 5.3.8 i 6.2.3
z książki
H.\,Hanche-Olsen, E.\,St{\o}rmer,
\emph{Jordan operator algebras} \cite{Hanche1984},
który mówi, że każda JB$^{*}$-algebra zawarta w $M_{3}$
jest izomorficzna z jedną z następujących algebr:
$\mathbb{C}\mathbf{1}$,
$\mathbb{C} P_{12} \oplus \mathbb{C} P_{3}$,
$\mathbb{C} P_{1} \oplus \mathbb{C} P_{2} \oplus \mathbb{C} P_{3}$,
$M_{2} \oplus \mathbb{C} P_{3}$,
$M_{2}^{s} \oplus P_{3}$,
$M_{3}^{s}$ oraz $M_{3}$,
gdzie $M_{n}^{s}$ jest algebrą Jordana składającą się z symetrycznych
macierzy rozmiaru $n$: $M_{n}^{s} = \{ A \in M_{n}: A = A^{t} \}$.

\begin{Theorem}
\label{thm:ExposedMaps}
Niech $S\!: M_{3} \rightarrow M_{3}$ będzie ekstremalnym odwzorowaniem
bistochastycznym.
Wówczas JB$^{*}$-algebra $K_{S}$ jest izomorficzna z jedną z następujących:
$\mathbb{C}\mathbf{1}$, $\mathbb{C} P_{12} \oplus \mathbb{C} P_{3}$
lub
$M_{3}$.
\end{Theorem}

\begin{proof}
\emph{Krok 1.}
Tezy twierdzenia dowiedziemy najpierw poprzez wykluczenie wszystkich
pozostałych przypadków algebr Jordana.
Niech $K_{S}$ będzie jedną z następujących JB$^{*}$-algebr:
$\mathbb{C} P_{1} \oplus \mathbb{C} P_{2} \oplus \mathbb{C} P_{3}$,
$M_{2} \oplus \mathbb{C} P_{3}$,
$M_{2}^{s} \oplus P_{3}$ albo $M_{3}^{s}$.
Wówczas $K_{S}$ zawiera projektory ortogonalne $P_{1}, P_{2}, P_{3}$,
a ponieważ $S$ jest automorfizmem Jordana na $K_{S}$ i $S$ zachowuje ślad:
\begin{linenomath*}
 \begin{multline}
    \text{Tr} \, S(P_{i}) S(P_{j}) = \\
= \frac{1}{2} \text{Tr} \, \left( S(P_{i}) S(P_{j}) + S(P_{j}) S(P_{i}) \right)=
\frac{1}{2} \text{Tr} \, S(P_{i} P_{j} + P_{j} P_{i})=
\text{Tr} \, P_{i} P_{j} = \delta_{ij},
 \end{multline}
\end{linenomath*}
gdzie $i,j = 1,2,3$ oraz $\delta_{ij}$ jest deltą Kroneckera.
Stąd, $\left\{S(P_{i})\right\}_{i=1}^{3}$
jest trójką ortogonalnych projektorów rzędu 1, prostopadłych względem siebie.
Istnieje więc macierz unitarna $U \in M_{3}$, taka że
$U^{*} S(P_{i}) U = P_{i}$, dla $i = 1,2,3$.
Zdefiniujmy $\tilde{S}(A) = U^{*} S(A) U$.
Wówczas $\tilde{S}$ jest odwzorowaniem ekstremalnym
(por. E.\,St{\o}rmer \cite{Stormer2013}, lemat 3.1.2b, str.\,27)
i bistochastycznym, dla którego
$\tilde{S}(P_{i}) = P_{i}$, $i = 1,2,3$.
Na mocy twierdzenia 4.1 z pracy
\cite{kye1995positive},
$\tilde{S}$ jest rozkładalne,
co stoi w sprzeczności z faktem, że $S$ jest ekstremalne albo $K_{S} \neq M_{3}$.

\emph{Krok 2.}
Mimo że łatwo wskazać takie odwzorowania ekstremalne, dla których
$K_{S}=M_{3}$ (np. $S(A) = U A U^{*}$ dla macierzy unitarnej $U$)
lub $K_{S}=\mathbb{C} \mathbf{1}$
(weźmy np. znane odwzorowanie Choi
\cite{choi1977extremal}, p. str. \pageref{eq:choi});
należy jeszcze podać przykład ekstremalnego odwzorowania bistochastycznego,
dla którego
$K_{S}= \mathbb{C} P_{12} \oplus \mathbb{C} P_{3}$.

Niech więc $S\!: M_{3} \rightarrow M_{3}$ będzie odwzorowaniem liniowym
zadanym przez
\begin{linenomath*}
 \begin{equation}
\label{eq:DefinitionOfS}
S(A) \:=\: \begin{pmatrix}
        \frac{1}{2}(a_{11} + a_{22}) & 0 & \frac{1}{\sqrt{2}} a_{13} \\
        0 & \frac{1}{2}(a_{11} + a_{22}) & \frac{1}{\sqrt{2}} a_{32} \\
        \frac{1}{\sqrt{2}} a_{31} & \frac{1}{\sqrt{2}} a_{23} & a_{33}
        \end{pmatrix},
 \end{equation}
\end{linenomath*}
dla
$A = \left( a_{ij} \right)_{i,j=1}^{3}
        \in M_{3}$.
Jeśli posłużymy się notacją
$A = \left( \begin{smallmatrix}
    B & \vec{u} \\
    \vec{w}^{t} & z
    \end{smallmatrix} \right)$,
dla $B \in M_{2}$,
$\vec{u},\vec{w} \in \mathbb{C}^{2}$ są wektorami kolumnowymi,
a $z \in \mathbb{C}$,
odwzorowanie $S$ działa jak
\begin{linenomath*}
 \begin{equation}
    S(A) \:=\: S \begin{pmatrix}
    B & \vec{u} \\
    \vec{w}^{t} & z
    \end{pmatrix} \: = \:
    \begin{pmatrix}
        \frac{1}{2} (\text{Tr} B) \, \mathbf{1}_{2} &
            \frac{1}{\sqrt{2}}(\hat{P}_{1} \vec{u} + \hat{P}_{2} \vec{w}) \\
        \frac{1}{\sqrt{2}}(\hat{P}_{1} \vec{w} + \hat{P}_{2} \vec{u})^{t} & z
    \end{pmatrix},
 \end{equation}
\end{linenomath*}
gdzie
$\hat{P}_{1} = \left( \begin{smallmatrix} 1 & 0 \\ 0 & 0 \end{smallmatrix} \right)$,
$\hat{P}_{2} = \left( \begin{smallmatrix} 0 & 0 \\ 0 & 1 \end{smallmatrix} \right)$
oraz $\mathbf{1}_{2}$ jest macierzą jednostkową w $M_{2}$.
Zakładając, że następny lemat jest prawdziwy,
ten przykład kończy dowód.
\end{proof}

\pagebreak
\begin{Lemma}
\label{lem:SIsExtremal}
$S$ jest odwzorowaniem bistochastycznym i ekstremalnym.
\end{Lemma}

\begin{proof}
Aby udowodnić, że $S$ jest odwzorowaniem dodatnim,
wystarczy pokazać, że dla każdego $\eta \in \mathbb{C}^{3}\backslash\{0\}$,
$SP_{\eta} \geq 0$,
gdzie $P_{\eta}$ jest dodatnim operatorem rzędu 1,
$P_{\eta} = \eta \eta^{*}$.
Niech więc
$\eta = (\eta_{1}, \eta_{2}, \eta_{3}) = (\vec{\eta}, \eta_{3})$,
$\vec{\eta} = (\eta_{1}, \eta_{2}) \in \mathbb{C}^{2}\backslash\{0\}$,
$\eta_{3} \in \mathbb{C}$.
Wówczas mamy
\begin{linenomath*}
 \begin{multline}
 SP_{\eta} \:=\:  S \begin{pmatrix}
    \vec{\eta} \vec{\eta}^{\,*} & \overline{\eta}_{3} \vec{\eta} \\
    \eta_{3} \vec{\eta}^{\,*}   & |\eta_{3}|^{2}
 \end{pmatrix} \: = \: \\
 = \begin{pmatrix}
  \frac{||\vec{\eta}||^{2}}{2} \mathbf{1}_{2} &
        \frac{1}{\sqrt{2}} \left ( \overline{\eta}_{3} \hat{P}_{1} \vec{\eta} +
          \eta_{3} \hat{P}_{2} \overline{\vec{\eta}} \right) \\
\frac{1}{\sqrt{2}} \left ( \overline{\eta}_{3} \hat{P}_{1} \vec{\eta} +
          \eta_{3} \hat{P}_{2} \overline{\vec{\eta}} \right)^{*} &
        |\eta_{3}|^{2}
 \end{pmatrix}.
 \end{multline}
\end{linenomath*}
Jeżeli $\eta_{3} = 0$, wtedy oczywiście $SP_{\eta} \geq 0$.
W przypadku, gdy $\eta_{3} \neq 0$,
biorąc dopełnienie Schura,
%(por. Theorem 1.12, str.\,34 w \cite{Zhang2006}),
dostajemy, że $SP_{\eta} \geq 0$, wtedy i tylko wtedy, gdy
\begin{linenomath*}
 \begin{equation}
\label{ieq:SchurForS}
   \left ( \overline{\eta}_{3} \hat{P}_{1} \vec{\eta} +
    \eta_{3} \hat{P}_{2} \overline{\vec{\eta}} \right)
   \left ( \overline{\eta}_{3} \hat{P}_{1} \vec{\eta} +
    \eta_{3} \hat{P}_{2} \overline{\vec{\eta}} \right)^{*}
    \: \leq \:
        |\eta_{3}|^{2} \, ||\vec{\eta}||^{2} \, \mathbf{1}_{2}.
 \end{equation}
\end{linenomath*}
Łatwo jednak zauważyć, że ta nierówność jest spełniona:
\begin{linenomath*}
 \begin{multline}
   \left ( \overline{\eta}_{3} \hat{P}_{1} \vec{\eta} +
    \eta_{3} \hat{P}_{2} \overline{\vec{\eta}} \right)
   \left ( \overline{\eta}_{3} \hat{P}_{1} \vec{\eta} +
    \eta_{3} \hat{P}_{2} \overline{\vec{\eta}} \right)^{*} \: \leq \:
|| \overline{\eta}_{3} \hat{P}_{1} \vec{\eta} +
    \eta_{3} \hat{P}_{2}\overline{\vec{\eta}} ||^{2} \, \mathbf{1}_{2}
    \: = \: \\
\left( || \overline{\eta}_{3} \hat{P}_{1} \vec{\eta} ||^{2} +
    || \eta_{3} \hat{P}_{2} \overline{\vec{\eta}} ||^{2} \right) \, \mathbf{1}_{2}
    \: = \:
|\eta_{3}|^{2} \, ||\vec{\eta}||^{2} \, \mathbf{1}_{2}.
 \end{multline}
\end{linenomath*}
Zatem $S$ jest odwzorowaniem bistochastycznym.
Ponadto,
$K_{S}= \mathbb{C} P_{12} \oplus \mathbb{C} P_{3}$.
Rzeczywiście,
z bezpośredniego rachunku wynika, że dla każdego $A \in M_{3}$:
\begin{linenomath*}
 \begin{equation}
\lim \limits_{k \rightarrow \infty} S^{k}(A) \:=\:
\frac{1}{2} (\text{Tr} \, P_{12} A )\, P_{12} +
    (\text{Tr} \, P_{3} A)\, P_{3} \, \in K_{S}.
 \end{equation}
\end{linenomath*}
Z definicji \eqref{def:definitionofK},
dostajemy postać JB$^{*}$-algebry $K_{S}$.

W następnej kolejności pokażemy, że $S$ jest ekstremalne
w zbiorze wszystkich odwzorowań dodatnich na $M_{3}$.
Niech $S_{0}: M_{3} \rightarrow M_{3}$ będzie dodatnim odwzorowaniem,
takim że
$0 \leq S_{0} \leq S$.
Mamy $S_{0}(P_{3}) \leq S(P_{3}) = P_{3}$,
a stąd $S_{0}(P_{3}) = \alpha P_{3}$ dla pewnego $0 \leq \alpha \leq 1$.
Przestrzeń $M_{2}$ możemy traktować jako zanurzoną w $M_{3}$
w następujący sposób:
$A \in M_{2} \! \subset \! M_{3}$, o ile
\begin{linenomath*}
 \begin{equation}
\label{RandomLabel:450031}
    A \: = \: \left( \begin{array}{ccc}
    a_{11} & a_{12} & 0 \\
    a_{21} & a_{22} & 0 \\
    0 & 0 & 0
    \end{array} \right), \quad
    a_{ij} \in \mathbb{C}, \,\, i,j = 1,2.
 \end{equation}
\end{linenomath*}
Chcemy pokazać, że $S_{0}(A) \in M_{2} \! \subset \! M_{3}$
dla każdego $A \in M_{2} \! \subset \! M_{3}$.
Niech $B \in M_{2}$; załóżmy ponadto z początku, że $B \geq 0$.
Wówczas
\begin{linenomath*}
 \begin{equation}
\label{eq:SMapsM2intoM2}
 0 \:\leq\: S_{0} \begin{pmatrix}
               B & \vec{0} \\ \vec{0}^{t} & 0
              \end{pmatrix} \: = \:
 \begin{pmatrix}
  \hat{S}_{0}(B) & \vec{u} \\ \vec{w}^{t} & r
 \end{pmatrix},
 \end{equation}
\end{linenomath*}
dla pewnych wektorów $\vec{u}, \vec{w} \in \mathbb{C}^{2}$ oraz $r \geq 0$.
Odwzorowanie $\hat{S}_{0}:B \mapsto \hat{S}_{0}(B) \in M_{2}$ jest
z konieczności dodatnie na $M_{2}$, a dodatkowo
$\hat{S}_{0}(B) \leq \frac{1}{2} (\text{Tr} B) \mathbf{1}_{2}$.
Z drugiej strony,
\begin{linenomath*}
 \begin{equation}
 0 \: \leq \:
 \begin{pmatrix}
  \hat{S}_{0}(B) & \vec{u} \\ \vec{w}^{t} & r
 \end{pmatrix} \: \leq \:
            S \begin{pmatrix}
               B & \vec{0} \\ \vec{0}^{t} & 0
              \end{pmatrix} \: = \:
 \begin{pmatrix}
  \frac{1}{2} (\text{Tr} B) \, \mathbf{1}_{2} & \vec{0} \\ \vec{0}^{t} & 0
 \end{pmatrix},
 \end{equation}
\end{linenomath*}
a zatem $r=0$, $\vec{u}=\vec{w}=\vec{0}$.
Ponieważ każda macierz $B \in M_{2}$ jest kombinacją liniową czterech macierzy
dodatnich, dostajemy w istocie, że
$S_{0}(A) \in M_{2} \! \subset \! M_{3}$
dla dowolnego $A \in M_{2} \! \subset \! M_{3}$.

Niech $\{e_{i}\}_{i=1}^{3}$ będzie standardową
bazą ortonormalną $\mathbb{C}^{3}$
oraz $\{ E_{jk} \}_{j,k=1}^{3}$ będzie zbiorem macierzy z $M_{3}$,
dla których $E_{jk} = e_{j} e_{k}^{*}$.
Pokażemy, że dla $i = 1,2,3$ oraz $j=1,2$;
$\langle e_{i}, S_{0}(E_{j3}) e_{i} \rangle = 0$
i $\langle e_{1}, S_{0}(E_{j3}) e_{2} \rangle =
    \langle e_{2}, S_{0}(E_{j3}) e_{1} \rangle = 0$.
Ustalmy $i,j$ oraz
$X = |z_{1}|^{2} P_{j} + z_{1} \overline{z_{2}} E_{j3} +
\overline{z_{1}} z_{2} E_{3j} + |z_{2}|^{2} P_{3}$,
dla pewnych $z_{1}, z_{2} \in \mathbb{C}$.
Jasne jest, że $X \geq 0$, a więc:
\begin{linenomath*}
 \begin{multline}
0 \: \leq \: \langle e_{i}, S_{0}(X) e_{i} \rangle \: = \:
|z_{1}|^{2} \, \langle e_{i}, S_{0}(P_{j}) e_{i} \rangle +\\
+ 2 \, \text{Re} \, z_{1} \overline{z_{2}} \,
    \langle e_{i}, S_{0}(E_{j3}) e_{i} \rangle +
|z_{2}|^{2} \, \langle e_{i}, S_{0}(P_{3}) e_{i} \rangle,
 \end{multline}
\end{linenomath*}
Jako że powyższe równanie jest prawdziwe dla każdej pary
$z_{1}, z_{2} \in \mathbb{C}$,
dostajemy, że poniższa macierz musi być dodatnio określona:
\begin{linenomath*}
\begin{equation}
  \begin{pmatrix}
    \langle e_{i}, S_{0}(P_{j}) e_{i} \rangle & \langle e_{i}, S_{0}(E_{j3}) e_{i} \rangle \\
    \langle e_{i}, S_{0}(E_{3j}) e_{i} \rangle & \delta_{i3},
  \end{pmatrix}
\end{equation}
\end{linenomath*}
skoro $\langle e_{i}, S_{0}(P_{3}) e_{i} \rangle = \delta_{i3}$.
Ponieważ $S_{0}(E_{j3}) = S_{0}(E_{3j})^{*}$,
obliczjąc wyznacznik tej macierzy, otrzymujemy:
$ | \langle e_{i}, S_{0}(E_{j3}) e_{i} \rangle |^{2} \leq \delta_{i3} \langle e_{i}, S_{0}(P_{j}) e_{i} \rangle$.
Z uwagi na to, że $j \neq 3$,
widzimy, że dla $i = 1,2,3$, $\langle e_{i}, S_{0}(E_{j3}) e_{i} \rangle = 0$.

Niech teraz $z_{1} =  \frac{1}{k}$ oraz $z_{2} = u k$ dla dowolnego
$k \in \mathbb{N}$ oraz $u \in \{ 1, i \}$, tzn. $|u| = 1, u^{2} = \pm 1$.
Wówczas
$S(X) = \frac{1}{k^{2}} S(P_{j}) + \bar{u} S(E_{j3}) + u S(E_{3j}) + k^{2} P_{3}$.
Ponieważ $S(X)$ jest dodatnio określoną macierzą, więc jej drugi główny minor
jest liczbą nieujemną:
\begin{linenomath*}
\begin{equation}
  \nonumber
    0 \leq \det \Bigg[ \frac{1}{k^{2}} \begin{pmatrix}
          \langle e_{1} , S(P_{j}) e_{1} \rangle &  \langle e_{1} , S(P_{j}) e_{2} \rangle  \\
          \langle e_{2} , S(P_{j}) e_{1} \rangle &  \langle e_{2} , S(P_{j}) e_{2} \rangle
                \end{pmatrix}
                +
\end{equation}
\end{linenomath*}
\begin{linenomath*}
\begin{equation}
                \begin{pmatrix}
          0 & \langle e_{1} , \bar{u} S(E_{j3}) + u S(E_{3j})  e_{2} \rangle \\
          \langle e_{2} , \bar{u} S(E_{j3}) + u S(E_{3j})  e_{1} \rangle & 0
              \end{pmatrix}
          \Bigg],
\end{equation}
\end{linenomath*}
dla każdego $k \in \mathbb{N}$,
co oznacza, że
\begin{linenomath*}
\begin{equation}
 \langle e_{1} , \bar{u} S(E_{j3}) + u S(E_{3j})  e_{2} \rangle \
\langle e_{2} , \bar{u} S(E_{j3}) + u S(E_{3j})  e_{1} \rangle  \leq 0,
\end{equation}
\end{linenomath*}
a ponieważ $S(E_{j3}) = S(E_{j3})^{*}$ oraz $u$ przyjmuje wartości $\pm 1$,
dostajemy, że
$\langle e_{1}, S(E_{j3}) e_{2} \rangle = \langle e_{2}, S(E_{j3}) e_{1} \rangle = 0$.

Na mocy powyższego możemy napisać:
$A  =   \left( \begin{smallmatrix}
    B & \vec{u} \\
    \vec{w}^{t} & z
    \end{smallmatrix} \right) \in M_{3}$,
\begin{linenomath*}
 \begin{equation}
    S_{0}(A) \:=\: S_{0} \begin{pmatrix}
    B & \vec{u} \\
    \vec{w}^{t} & z
    \end{pmatrix} \: = \:
    \begin{pmatrix}
        \hat{S}_{0}(B) & S_{1} \vec{u} + S_{2} \vec{w} \\
        (\overline{S}_{1} \vec{w} + \overline{S}_{2} \vec{u})^{t} & \alpha z
    \end{pmatrix}
 \end{equation}
\end{linenomath*}
oraz  $S_{1}, S_{2} \in M_{2}$, których elementy macierzowe dane są przez
\begin{linenomath*}
 \begin{equation}
(S_{1})_{ij} = \langle e_{i}, S(E_{j3}) e_{3} \rangle, \quad \quad
(S_{2})_{ij} = \langle e_{i}, S(E_{3j}) e_{3} \rangle.
 \end{equation}
\end{linenomath*}

Załóżmy, że $\alpha = 0$.
Wówczas, skoro $S_{0} P_{\eta} \geq 0$ dla każdego
$\eta \in \mathbb{C}^{3}\backslash\{0\}$,
mamy $S_{1} = S_{2} = 0$.
Ponieważ
$0 \leq (S - S_{0}) P(1,1,1)$,
więc korzystając z dopełnienia Schura:
\begin{linenomath*}
 \begin{equation}
0 \: \leq \: \hat{S}_{0} \begin{pmatrix}
    1 & 1 \\ 1 & 1
    \end{pmatrix} \: \leq \:
    \mathbf{1}_{2} - \frac{1}{2} \begin{pmatrix}
                1 & 1 \\ 1 & 1
                \end{pmatrix} \: = \:
    \frac{1}{2}
    \begin{pmatrix}
    1 & -1 \\ -1 & 1
    \end{pmatrix},
 \end{equation}
\end{linenomath*}
tzn. dla pewnego $\beta \geq 0$,
$
\hat{S}_{0} \left( \begin{smallmatrix}
    1 & 1 \\ 1 & 1
    \end{smallmatrix} \right) =
    \beta \left(
    \begin{smallmatrix}
    1 & -1 \\ -1 & 1
    \end{smallmatrix} \right)
$.
Powtarzając ten sam rachunek, tym razem dla $P(i,i,1)$,
dostajemy, że dla pewnego $\beta' \geq 0$:
$
\hat{S}_{0} \left( \begin{smallmatrix}
    1 & 1 \\ 1 & 1
    \end{smallmatrix} \right) =
    \beta' \left(
    \begin{smallmatrix}
    1 & 1 \\ 1 & 1
    \end{smallmatrix} \right)
$,
a zatem
$
\hat{S}_{0} \left( \begin{smallmatrix}
    1 & 1 \\ 1 & 1
    \end{smallmatrix} \right) = 0
$.
Podobnie:
$
\hat{S}_{0} \left( \begin{smallmatrix}
    1 & -1 \\ -1 & 1
    \end{smallmatrix} \right) = 0
$, a stąd
$\hat{S}_{0}(\mathbf{1}_{2}) = 0$, tzn. $\hat{S}_{0} = 0$.
Zatem możemy założyć w dalszym toku, że $\alpha > 0$.

Bezpośrednim rachunkiem dowodzimy, że dla każdego
$\vec{\eta} \in \mathbb{C}^{2}$:
\begin{linenomath*}
 \begin{equation}
 \label{eq:ZeroTrace}
 \text{Tr} \, P(-2\vec{\upsilon}, ||\vec{\eta}||^{2}) \, S P(\vec{\eta},1)
     \:=\: 0,
 \end{equation}
\end{linenomath*}
gdzie $\vec{\upsilon} =
 \frac{1}{\sqrt{2}} \left(
  \hat{P}_{1} \vec{\eta} + \hat{P}_{2} \overline{\vec{\eta}}
 \right)$,
$||\vec{\upsilon}||^{2} = \tfrac{||\vec{\eta}||^{2}}{2}$.
Ponieważ $0 \leq S_{0} \leq S$, równanie
\eqref{eq:ZeroTrace} zachodzi także dla $S_{0}$.
Rozpisując to równanie oraz zakładając, że $||\vec{\eta}|| = 1$,
dostajemy
\begin{linenomath*}
 \begin{equation}
 \label{eq:ZeroTraceExplicit}
 4 \, \vec{\upsilon}^{\,*} \hat{S}_{0}(\vec{\eta} \vec{\eta}^{\,*}) \vec{\upsilon} -
 4 \, \text{Re} \, \vec{\upsilon}^{\,*} \vec{\upsilon}_{0} + \alpha \: = \: 0,
 \end{equation}
\end{linenomath*}
gdzie $\vec{\upsilon_{0}} = S_{1} \vec{\eta} + S_{2} \overline{\vec{\eta}}$.
Aby nie komplikować niepotrzebnie zapisu,
pamiętajmy, że $\vec{\upsilon}$ i $\vec{\upsilon}_{0}$ zależą od $\vec{\eta}$,
bez zaznaczania tego jawnie.

Ponieważ $S_{0} P(\vec{\eta},1)$ jest macierzą dodatnią,
korzystając raz jeszcze z dopełnienia Schura, mamy
$\vec{\upsilon}_{0} \vec{\upsilon}_{0}^{\,*} \leq  \alpha \hat{S}_{0}(\vec{\eta} \vec{\eta}^{\,*})$,
a przez to
$|\vec{\upsilon}^{\,*} \vec{\upsilon}_{0}|^{2} \leq
  \alpha \vec{\upsilon}^{\,*} \hat{S}_{0}(\vec{\eta} \vec{\eta}^{\,*}) \vec{\upsilon}$.
Zatem
\begin{linenomath*}
 \begin{equation}
4 \left( \text{Im} \, \vec{\upsilon}^{\,*} \vec{\upsilon}_{0} \right)^{2} +
\left( 2 \, \text{Re} \, \vec{\upsilon}^{\,*} \vec{\upsilon}_{0} - \alpha \right)^{2}
 \: \leq \:
4 \alpha \, \vec{\upsilon}^{\,*} \hat{S}_{0}(\vec{\eta} \vec{\eta}^{\,*}) \vec{\upsilon}
    - 4 \alpha \, \text{Re} \, \vec{\upsilon}^{\,*} \vec{\upsilon}_{0} + \alpha^{2}  = 0,
 \end{equation}
\end{linenomath*}
tj. $\vec{\upsilon}^{\,*} \vec{\upsilon}_{0} = \frac{\alpha}{2}$.
Podstawiając $\vec{\upsilon}$ i $\vec{\upsilon}_{0}$,
otrzymujemy:
\begin{linenomath*}
 \begin{equation}
\label{eq:ComplicatedForEtaPlus}
\vec{\eta}^{\,*} \left( \hat{P}_{1} S_{1} + S_{2}^{t} \hat{P}_{2} \right) \vec{\eta}
    + \vec{\eta}^{\,*} \hat{P}_{1} S_{2} \overline{\vec{\eta}}
    + \vec{\eta}^{\,t} \hat{P}_{2} S_{1} \vec{\eta} \: = \: \frac{\alpha}{\sqrt{2}}.
 \end{equation}
\end{linenomath*}
Skoro równanie jest prawdziwe dla każdego  $\vec{\eta} \in \mathbb{C}^{2}$,
$|| \vec{\eta} || = 1$,
możemy powtórzyć całą argumentację, tym razem zamieniając
$\vec{\eta} \mapsto i \vec{\eta}$,
i otrzymać:
\begin{linenomath*}
 \begin{equation}
\label{eq:ComplicatedForEtaMinus}
\vec{\eta}^{\,*} \left( \hat{P}_{1} S_{1} + S_{2}^{t} \hat{P}_{2} \right) \vec{\eta}
    - \vec{\eta}^{\,*} \hat{P}_{1} S_{2} \overline{\vec{\eta}}
    - \vec{\eta}^{\,t} \hat{P}_{2} S_{1} \vec{\eta} \: = \: \frac{\alpha}{\sqrt{2}}.
 \end{equation}
\end{linenomath*}
Dodając do siebie
\eqref{eq:ComplicatedForEtaPlus} i \eqref{eq:ComplicatedForEtaMinus},
mamy:
$
\vec{\eta}^{\,*} \left( \hat{P}_{1} S_{1} + S_{2}^{t} \hat{P}_{2} \right) \vec{\eta} =
    \frac{\alpha}{\sqrt{2}},
$
dla dowolnego $\vec{\eta}$ o normie jednostkowej.
Stąd
\begin{linenomath*}
 \begin{equation}
\label{eq:S1PlusS2EqualsOne}
\hat{P}_{1} S_{1} + S_{2}^{t} \hat{P}_{2} \: = \:
     \frac{\alpha}{\sqrt{2}} \mathbf{1}_{2}.
 \end{equation}
\end{linenomath*}
Odejmując \eqref{eq:ComplicatedForEtaMinus} od
\eqref{eq:ComplicatedForEtaPlus},
dostajemy
\begin{linenomath*}
 \begin{equation}
    \vec{\eta}^{\,*} \hat{P}_{1} S_{2} \overline{\vec{\eta}}
    + \vec{\eta}^{\,t} \hat{P}_{2} S_{1} \vec{\eta} \: = \: 0.
 \end{equation}
\end{linenomath*}
Niech  $\vec{\eta}$ będzie po kolei jednym z następujących wektorów:
$(1,0)$, $(0,1)$,
$(i,0)$, $(0,i)$,
$(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$,
$(\frac{1}{\sqrt{2}},\frac{i}{\sqrt{2}})$.
Otrzymujemy, że $\hat{P}_{1} S_{2} = \hat{P}_{2} S_{1} = 0$.
Łącząc to z \eqref{eq:S1PlusS2EqualsOne},
dostajemy
\begin{linenomath*}
 \begin{equation}
S_{1} = \begin{pmatrix}
    \frac{\alpha}{\sqrt{2}} & z_{0} \\ 0 & 0
\end{pmatrix}, \quad
S_{2} = \begin{pmatrix}
     0 & 0 \\ - z_{0} & \frac{\alpha}{\sqrt{2}}
\end{pmatrix}, \quad
z_{0} \in \mathbb{C}.
 \end{equation}
\end{linenomath*}

Kładąc $\vec{\upsilon}^{\,*} \vec{\upsilon}_{0} = \frac{\alpha}{2}$
w równaniu \eqref{eq:ZeroTraceExplicit}, mamy
$
\vec{\upsilon}^{\,*} \hat{S}_{0}(\vec{\eta} \vec{\eta}^{\,*}) \vec{\upsilon} =
\frac{\alpha}{4}
$.
Stąd
\begin{linenomath*}
 \begin{equation}
 \vec{\upsilon}^{\,*} \left(
    \alpha \, \hat{S}_{0}(\vec{\eta} \vec{\eta}^{\,*}) -\vec{\upsilon}_{0} \vec{\upsilon}_{0}^{\,*}
  \right) \vec{\upsilon} \: = \: 0,
 \end{equation}
\end{linenomath*}
a ponieważ
$\vec{\upsilon}_{0} \vec{\upsilon}_{0}^{\,*} \leq  \alpha \hat{S}_{0}(\vec{\eta} \vec{\eta}^{\,*})$,
musi być tak, że
$ \alpha \, \hat{S}_{0}(\vec{\eta} \vec{\eta}^{\,*}) \vec{\upsilon} =
\left( \vec{\upsilon}_{0}^{\,*} \vec{\upsilon} \right)  \vec{\upsilon}_{0}$
lub prościej:
$ \hat{S}_{0}(\vec{\eta} \vec{\eta}^{\,*}) \vec{\upsilon} = \frac{1}{2} \vec{\upsilon}_{0}$.
Raz jeszcze powtórzmy całą argumentację, zamieniając
$\vec{\eta} \mapsto i \vec{\eta}$,
dodając otrzymany wynik i odejmując od poprzedniego.
Ostatecznie dostajemy:
\begin{linenomath*}
 \begin{subequations}
    \begin{align}
\label{eq:SZeroIsAlmostOneA}
\eta_{1} \, \hat{S}_{0}(\vec{\eta} \vec{\eta}^{\,*}) \vec{e}_{1} \: = \:
   \left( \frac{\alpha}{2} \eta_{1} + \frac{\sqrt{2}}{2} z_{0} \eta_{2} \right) \, \vec{e}_{1}, \\
\label{eq:SZeroIsAlmostOneB}
\overline{\eta}_{2} \, \hat{S}_{0}(\vec{\eta} \vec{\eta}^{\,*}) \vec{e}_{2} \: = \:
   \left(- \frac{\sqrt{2}}{2} z_{0} \overline{\eta}_{1} + \frac{\alpha}{2} \overline{\eta}_{2} \right)
       \, \vec{e}_{2},
    \end{align}
 \end{subequations}
\end{linenomath*}
gdzie $\vec{\eta} = (\eta_{1}, \eta_{2})$ oraz
$\vec{e}_{1} = (1,0), \vec{e}_{2} = (0,1) \in \mathbb{C}^{2}$.
Wykorzystując np. \eqref{eq:SZeroIsAlmostOneA}
i mnożąc przez  $\overline{\eta}_{1} \vec{e}_{1}^{\,*}$,
mamy:
\begin{linenomath*}
 \begin{equation}
0 \: \leq \:
 |\eta_{1}|^{2} \, \vec{e}_{1}^{\,*} \, \hat{S}_{0}(\vec{\eta} \vec{\eta}^{\,*}) \vec{e}_{1}
 \: = \:
\frac{\alpha}{2} |\eta_{1}|^{2} + \frac{\sqrt{2}}{2} z_{0} \overline{\eta}_{1} \eta_{2},
 \end{equation}
\end{linenomath*}
dla dowolnego $\vec{\eta} \in \mathbb{C}^{2}$, $||\vec{\eta}|| =1$.
Zatem $z_{0} = 0$ i
$S_{1} = \frac{\alpha}{2} \hat{P}_{1}$,
$S_{2} = \frac{\alpha}{2} \hat{P}_{2}$.
Jako wniosek z równań
\eqref{eq:SZeroIsAlmostOneA} i \eqref{eq:SZeroIsAlmostOneB},
$\hat{S}_{0} (\vec{\eta} \vec{\eta}^{\,*}) = \frac{\alpha}{2} \mathbf{1}_{2}$
dla każdego $\vec{\eta}$, takiego że $\eta_{1} \neq 0$ oraz $\eta_{2} \neq 0$.
Dla $0 \!<\!\epsilon \!<\! 1$,
weźmy $\vec{\eta}_{\epsilon} = (\sqrt{1 - \epsilon^{2}}, \epsilon)$.
Wówczas $\hat{S}_{0} (\hat{P}_{1}) =
\lim_{\epsilon \rightarrow 0} \hat{S}_{0}
    (\vec{\eta}_{\epsilon} \vec{\eta}_{\epsilon}^{\,*}) =
\frac{\alpha}{2} \mathbf{1}_{2}$.
Podobnie,
$\hat{S}_{0} (\hat{P}_{2}) = \frac{\alpha}{2} \mathbf{1}_{2}$,
co skutkuje tym, że
$\hat{S}_{0} (\vec{\eta} \vec{\eta}^{\,*}) = \frac{\alpha}{2} \mathbf{1}_{2}$
dla każdego $\vec{\eta} \in \mathbb{C}^{2}$,
$||\vec{\eta}|| = 1$.
To wystarczy, aby powiedzieć, że
$\hat{S}_{0}(B) = \frac{\alpha}{2} (\text{Tr} B) \mathbf{1}_{2}$
dla dowolnego $B \in M_{2}$.
Pokazaliśmy, że dla każdego dodatniego odwzorowania $S_{0}$, takiego że
$0 \leq S_{0} \leq S$, mamy
$S_{0} = \alpha S$ dla $0 \leq \alpha \leq 1$,
co oznacza, że $S$ jest ekstremalnym odwzorowaniem w stożku wszystkich
odwzorowań dodatnich na $M_{3}$ i co kończy dowód.
\end{proof}

Interesujące może być ponadto zauważenie, że $S$ nie jest odwzorowaniem
\mbox{2-dodatnim}.
Rzeczywiście,
możemy pokazać nawet więcej i uzasadnić, że $S$
nie spełnia nierówności Kadisona-Schwarza dla każdej macierzy
$B \in M_{3}$ (por. stwierdzenie \,4.1 \cite{choi1980some}).
Niech
$B = P_{12} + E_{32}$.
Wówczas łatwo pokazać, że
$S(B^{*} B) - S(B)^{*} S(B)$
nie jest dodatnie.
Oczywiście, odwzorowanie $S$ nie jest również 2-\emph{ko}dodatnie:
aby to zobaczyć, weźmy $B = P_{12} + E_{31}$.
Stąd, ponieważ $S$ jest ekstremalne,
wynika, że jest również odwzorowaniem atomowym
\cite{ha1998atomic}.

W celu udowodnienia ekstremalności $S$,
pokazaliśmy, że $S_{0} = \alpha S$,
dla każdego $0 \leq S_{0} \leq S$.
W rzeczywistości wystarczyło wykorzystać słabsze założenie.
Przyjmijmy, że $S_{0} \in \mathcal{P}(M_{3})$ oraz
$\text{Tr} P_{\xi} \, S_{0}(P_{\eta}) = 0$,
dla każdego $\mathbb{C}^{3} \ni \xi,\eta \neq 0$,
takiego że $\text{Tr} P_{\xi} \, S(P_{\eta}) = 0$.
Wówczas $\text{Tr} P_{12} \, S_{0}(P_{3}) = 0$ i
$\text{Tr} P_{3} \, S_{0}(P_{12}) = 0$.
Ponieważ $S_{0}$ jest dodatnie, więc $S_{0}(P_{3}) = \alpha P_{3}$
i $S_{0}(P_{12}) \in M_{2} \! \subset \! M_{3}$.
Dalej dowód można przeprowadzić dokładnie tak samo, jak w
lemacie \ref{lem:SIsExtremal},
poczynając od \eqref{eq:SMapsM2intoM2}.
To dowodzi zatem, że $S$ jest odwzorowaniem nie tylko ekstremalnym,
ale również ,,wystającym'' (ang. \emph{exposed}).

\vspace{0.5cm}

Mając dane odwzorowanie dodatnie $S: M_{n} \rightarrow M_{n}$,
świadek splątania związany z $S$ to macierz
$W_{S} \in M_{n^{2}} = M_{n} \! \otimes \! M_{n}$,
zdefiniowana jako
\begin{linenomath*}
 \begin{equation}
\label{def:entanglement-witness}
    W_{S} = \sum \limits_{i,j =1}^{n} E_{ij} \otimes S(E_{ij}),
 \end{equation}
\end{linenomath*}
gdzie $\{ E_{ij}\}_{i,j=1}^{n}$ to macierze posiadające wszystkie
zerowe elementy, oprócz elementu $ij$ równego jedności.
Twierdzenie udowodnione przez Choi i Jamiołkowskiego
\cite{choi1975completely,jamiolkowski1974effective}
mówi, że macierz $W_{S}$ jest dodatnim elementem $M_{n^{2}}$,
wtedy i tylko wtedy, gdy $S$ jest kompletnie dodatnie.
Zatem, dla dodatniego, ale nie kompletnie dodatniego odwzorowania $S$,
istnieje przynajmniej jedna macierz gęstości $\rho \in M_{n^{2}}$,
taka że $\text{Tr}\, W_{S} \rho < 0$.
Taka macierz nie może być separowalna \cite{werner1989quantum},
a więc możemy powiedzieć, że świadek splątania \emph{wykrywa}
stan splątany $\rho$.

Interesujące może być pokazanie, z jakiej postaci świadkiem splątania
stowarzyszone jest odwzorowanie zadane w równaniu \eqref{eq:DefinitionOfS},
oraz podanie rodziny stanów splątanych na
$M_{9} = M_{3} \! \otimes \! M_{3}$, wykrywanych przez $S$.
W tym przypadku:
\begin{linenomath*}
 \begin{equation}
\label{WS-C}
 W_S =  \left( \begin{array}{ccc|ccc|ccc}
 \frac{1}{2} &  \cdot& \cdot& \cdot& \cdot& \cdot& \cdot& \cdot& \frac{1}{\sqrt{2}} \\
 \cdot& \frac{1}{2} &\cdot& \cdot& \cdot& \cdot& \cdot& \cdot& \cdot\\
 \cdot& \cdot& \cdot & \cdot& \cdot& \cdot& \cdot& \cdot& \cdot  \\ \hline
 \cdot& \cdot& \cdot& \frac{1}{2} & \cdot& \cdot& \cdot& \cdot&  \cdot \\
 \cdot& \cdot& \cdot& \cdot& \frac{1}{2} & \cdot& \cdot& \cdot&  \cdot \\
 \cdot& \cdot& \cdot& \cdot& \cdot& \cdot& \cdot & \frac{1}{\sqrt{2}}& \cdot  \\ \hline
 \cdot& \cdot& \cdot & \cdot& \cdot& \cdot& \cdot& \cdot& \cdot  \\
 \cdot & \cdot& \cdot& \cdot& \cdot& \frac{1}{\sqrt{2}}& \cdot& \cdot& \cdot \\
 \frac{1}{\sqrt{2}}& \cdot& \cdot& \cdot& \cdot & \cdot& \cdot& \cdot& 1
  \end{array} \right),
 \end{equation}
\end{linenomath*}
gdzie dla czytelności oznaczyliśmy elementy zerowe macierzy przez kropki.
Macierz $W_{S}$ nie jest dodatnią macierzą w
$M_{9} = M_{3} \! \otimes \! M_{3}$.
Niech $U \in \text{U}(9)$ będzie macierzą unitarną, dla której
$U^{*} W_{S} U = W_{S}^{(2)} \oplus W_{S}^{(7)}$,
gdzie
\begin{linenomath*}
\begin{equation}
\label{WS-DirectSum}
 W_{S}^{(2)} = \left(\begin{array}{cc}
    0 & \frac{1}{\sqrt{2}}  \\ \frac{1}{\sqrt{2}}  & 0
  \end{array}\right) \in M_{2}
 \end{equation}
\end{linenomath*}
\begin{linenomath*}
\begin{equation}
 W_S^{(7)}\ = \  \left( \begin{array}{ccccccc}
 \frac{1}{2} &  \cdot& \cdot& \cdot& \cdot& \cdot& \frac{1}{\sqrt{2}} \\
 \cdot& \frac{1}{2} &\cdot& \cdot& \cdot& \cdot& \cdot\\
 \cdot& \cdot& \cdot & \cdot& \cdot& \cdot& \cdot \\
 \cdot& \cdot& \cdot& \frac{1}{2} & \cdot&  \cdot& \cdot \\
 \cdot& \cdot& \cdot& \cdot& \frac{1}{2} & \cdot& \cdot \\
 \cdot& \cdot& \cdot & \cdot& \cdot& \cdot& \cdot  \\
 \frac{1}{\sqrt{2}}& \cdot& \cdot& \cdot& \cdot& \cdot& 1
  \end{array} \right) \in M_{7}.
 \end{equation}
\end{linenomath*}
Jasne jest, że $W_{S}^{(7)} \geq 0$.
Niech także  $\vec{v}$ będzie jednym z wektorów własnych $W_{S}^{(2)}$:
$\vec{v} = \frac{1}{\sqrt{2}} (1,-1)^{t} \in \mathbb{C}^{2}$,
$W_{S}^{(2)} \vec{v} = - \frac{1}{\sqrt{2}} \vec{v}$ oraz niech
$P_{\vec{v}} = \vec{v} \vec{v}^{*}$
będzie projektorem ortogonalnym na jedno wymiarową podprzestrzeń
rozpiętą przez $\vec{v}$.
Zdefiniujmy
$\rho = \frac{1}{2} U ( P_{\vec{v}} \oplus \rho_{0} ) U^{*}$,
dla macierzy gęstość $\rho_{0} \in M_{7}$,
$\rho_{0} \geq 0$, $\text{Tr} \rho_{0} = 1$,
takiej że
$\text{Tr}\, W_{S}^{(7)} \rho_{0} < \frac{1}{\sqrt{2}}$.
Wówczas $\rho \geq 0$ i $\text{Tr} \rho = 1$,
tzn. $\rho$ jest macierzą gęstości.
Co więcej,
\begin{linenomath*}
 \begin{multline}
\label{RandomLabel:851252}
    \text{Tr}\, W_{S} \rho =
\frac{1}{2} \text{Tr}\, W_{S} U (P_{\vec{v}} \oplus \rho_{0}) U^{*} =
\frac{1}{2} \text{Tr}\, (W_{S}^{(2)} \oplus W_{S}^{(7)}) (P_{\vec{v}} \oplus \rho_{0}) = \\ =
\frac{1}{2} \text{Tr}\, W_{S}^{(2)} P_{\vec{v}} + \frac{1}{2} \text{Tr}\, W_{S}^{(7)} \rho_{0}
< 0,
 \end{multline}
\end{linenomath*}
co oznacza, że $W_{S}$ wykrywa stan splątany $\rho$.
W szczególności stan $\rho$ zadany przez
\begin{linenomath*}
 \begin{equation}
\label{PPTstate}
 \rho =  \frac{1}{7} \left( \begin{array}{ccc|ccc|ccc}
 1 &  \cdot& \cdot& \cdot& \cdot& \cdot& \cdot& \cdot& -1 \\
 \cdot& \cdot& \cdot& \cdot& \cdot& \cdot& \cdot& \cdot& \cdot\\
 \cdot& \cdot& 1 & \cdot& \cdot& \cdot& \cdot& \cdot& \cdot  \\ \hline
 \cdot& \cdot& \cdot& \cdot& \cdot& \cdot& \cdot& \cdot&  \cdot \\
 \cdot& \cdot& \cdot& \cdot& 1 & \cdot& \cdot& \cdot&  \cdot \\
 \cdot& \cdot& \cdot& \cdot& \cdot& 1 & \cdot & -1 & \cdot  \\ \hline
 \cdot& \cdot& \cdot & \cdot& \cdot& \cdot& 1& \cdot& \cdot  \\
 \cdot & \cdot& \cdot& \cdot& \cdot& -1 & \cdot& 1 & \cdot \\
 -1 & \cdot& \cdot& \cdot& \cdot & \cdot& \cdot& \cdot& 1
  \end{array} \right)
 \end{equation}
\end{linenomath*}
jest stanem PPT, tzn. $(I \otimes t)\rho$ jest wciąż macierzą gęstości,
jednak $\text{Tr}\, W_{S} \rho = \frac{2}{7} - \frac{2\sqrt{2}}{7} < 0$.

\paragraph{}

Dla odwzorowania $S$ zadanego przez \eqref{eq:DefinitionOfS}, mamy
$K_{S}= \mathbb{C} P_{12} \oplus \mathbb{C} P_{3}$.
Według autora, jest to jak dotąd jedyny znany przykład ekstremalnego
odwzorowania na $M_{3}$,
które posiada tę właśnie stabilną podprzestrzeń $K_S$
(por. komentarz poniżej na str. \pageref{com:onlyOneExample}).

W następnym podrozdziale spróbujemy wskazać na topologiczne własności
odwzorowań dodatnich na $M_3$, które pochodzą z uogólnienia wprowadzonych
powyżej metod dla dwuwymiarowego przypadku
i które pozwolą nam przeprowadzić wstępną klasyfikację ekstremalnych
odwzorowań bistochastycznych.

\section{Ekstremalne odwzorowania dodatnie na $M_{3}$
a macierze idempotentne}

Podobnie, jak powyżej w przypadku odwzorowań dodatnich algebry $M_{2}$,
zdefiniujmy reprezentację odwzorowań na $M_{3}$ poprzez macierze
rzeczywiste działające na wektory bazowe w tej przestrzeni,
traktowanej jako przestrzeń Hilberta z iloczynem skalarnym Hilberta-Schmidta.

Wybierzmy znormalizowany zbiór macierzy Gell-Manna
$\left \{ \lambda_{\mu} \right \}_{\mu=0}^{8}$:
\begin{linenomath*}
 \begin{equation}
\nonumber
    \lambda_{1} = \frac{1}{\sqrt{2}} \begin{pmatrix}
            0 & 1 & 0 \\
            1 & 0 & 0 \\
            0 & 0 & 0
        \end{pmatrix}, \quad
    \lambda_{2} = \frac{1}{\sqrt{2}} \begin{pmatrix}
            0 & -i & 0 \\
            i & 0 & 0 \\
            0 & 0 & 0
        \end{pmatrix}, \quad
    \lambda_{3} = \frac{1}{\sqrt{2}} \begin{pmatrix}
            1 & 0 & 0 \\
            0 & -1 & 0 \\
            0 & 0 & 0
        \end{pmatrix},
 \end{equation}
\end{linenomath*}
\begin{linenomath*}
 \begin{equation}
\nonumber
    \lambda_{4} = \frac{1}{\sqrt{2}} \begin{pmatrix}
            0 & 0 & 1 \\
            0 & 0 & 0 \\
            1 & 0 & 0
        \end{pmatrix}, \quad
    \lambda_{5} = \frac{1}{\sqrt{2}} \begin{pmatrix}
            0 & 0 & -i \\
            0 & 0 & 0 \\
            i & 0 & 0
        \end{pmatrix}, \quad
    \lambda_{6} = \frac{1}{\sqrt{2}} \begin{pmatrix}
            0 & 0 & 0 \\
            0 & 0 & 1 \\
            0 & 1 & 0
        \end{pmatrix},
 \end{equation}
\end{linenomath*}
\begin{linenomath*}
 \begin{equation}
    \lambda_{7} = \frac{1}{\sqrt{2}} \begin{pmatrix}
            0 & 0 & 0 \\
            0 & 0 & -i \\
            0 & i & 0
        \end{pmatrix}, \quad
    \lambda_{8} = \frac{1}{\sqrt{6}} \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & -2
        \end{pmatrix}
 \end{equation}
\end{linenomath*}
oraz $\lambda_{0} = \frac{1}{\sqrt{3}} \mathbf{1}_{3}$.
Każda macierz hermitowska $A = A^{*} \in M_{3}$ może zostać przedstawiona
jako rzeczywista kombinacja liniowa macieży bazowych $A = \lambda(a)$,
gdzie $\lambda(a) = \sum_{\mu=0}^{8} a_{\mu} \lambda_{\mu}$
oraz
$a = (a_{0}, a_{1}, \ldots, a_{8}) \in \mathbb{R}^{9}$.
Czasem będziemy używać notacji:
$a = (a_{0}, \vec{a})$,
gdzie $\vec{a} = (a_{1},\ldots,a_{8}) \in \mathbb{R}^{8}$,
oraz $\lambda(a) = \lambda(a_{0}, \vec{a}) = a_{0} \lambda_{0} + \vec{a} \cdot \vec{\lambda}$.
Dla odwzorowania bistochastycznego $S$,
podobnie jak powyżej w przypadku przekształceń przestrzeni $M_{2}$
(p. str. \pageref{def:PiofSiso}),
zdefiniujmy macierz $x = (x_{ij})_{i,j=1}^{8} \in M_{8}(\mathbb{R})$ poprzez
$x_{ij} = \text{Tr} \lambda_{i} S(\lambda_{j})$,
$i,j = 1,2,\ldots,8$.
Wówczas łatwo zobaczyć, że $S$ działa na macierze samosprzężone jak
\begin{linenomath*}
 \begin{equation}
\label{eq:isomoprhism}
S(\lambda(a_{0}, \vec{a})) = \lambda(a_{0}, x \vec{a}).
 \end{equation}
\end{linenomath*}
Dla macierzy $x \in M_{8}(\mathbb{R})$,
oznaczmy odwzorowanie liniowe na $M_{3}$,
zachowujące ślad i macierz jednostkową
oraz zdefiniowane przez równanie \eqref{eq:isomoprhism},
przez $S_{x}$.
Niech $\Lambda \subset M_{8}(\mathbb{R})$ będzie zbiorem tych macierzy
rzeczywistych $x$,
dla których $S_{x}$ jest odwzorowaniem bistochastycznym:
\begin{linenomath*}
 \begin{equation}
\Lambda = \left \{ x \in M_{8}(\mathbb{R}) \: |  \: S_{x} \geq 0 \right \}.
 \end{equation}
\end{linenomath*}
Zatem, istnieje wzajemnie jednoznaczna odpowiedniość pomiędzy $\Lambda$
i zbiorem odwzorowań bistochastycznych algebry $M_{3}$.
Odwzorowanie $S_{x} \mapsto x$ jest izomorfizmem półgrup,
dla którego  $S_{x}^{*} = S_{x^{t}}$.
Niestety, struktura zbioru $\Lambda$ jest skomplikowana.
Po pierwsze, jest to zbiór wypukły,
tzn. dla każdego $x,y \in \Lambda$,
$\lambda x + (1-\lambda)y \in \Lambda$,
dla $0 \leq \lambda \leq 1$.
Co więcej, jest to zwarta afiniczna półgrupa topologiczna
(por. \cite{schwarz1955hausdorff,chow1975compact})
z inwolucją zadaną przez transpozycję macierzy: $A \mapsto A^{t}$.

\begin{Theorem}
\label{thm:LambdaAndBalls}
Niech $\overline{K}_{r}(\hat{0})$ oznacza domkniętą kulę jednostkową
w $M_{8}(\mathbb{R})$, względem normy operatorowej,
o środku w $\hat{0}$ i promieniu $r > 0$.
Wówczas
\begin{linenomath*}
 \begin{equation}
        \overline{K}_{\frac{1}{2}}(\hat{0})
            \subset \Lambda \subset \overline{K}_{1}(\hat{0}).
     \end{equation}
\end{linenomath*}
\end{Theorem}
\begin{proof}
    Załóżmy, że $x \in M_{8}(\mathbb{R})$ oraz $||x|| \leq \frac{1}{2}$.
    Niech $\vec{m}, \vec{n} \in \mathbb{R}^{8}$ będą takie, że
    $P_{\vec{m}} = \lambda(\frac{1}{\sqrt{3}}, \vec{n})$ i
    $P_{\vec{n}} = \lambda(\frac{1}{\sqrt{3}}, \vec{m})$
    są projektorami ortogonalnymi w $M_{3}$.
    Łatwo sprawdzić, że w tym przypadku
    $|| \vec{m} ||_{2} = || \vec{n} ||_{2} = \sqrt{\frac{2}{3}}$,
    gdzie  $|| \cdot||_{2}$ oznacza standardową normę euklidesową.
    Ponieważ $||x|| \leq \frac{1}{2}$, więc
\begin{linenomath*}
 \begin{equation}
        \text{Tr} \, P_{\vec{m}} S_{x}(P_{\vec{n}}) =
        \frac{1}{3} + \langle \vec{m}, x \vec{n} \rangle \geq
        \frac{1}{3} - |\langle \vec{m}, x \vec{n} \rangle| \geq 0.
        %\frac{1}{3} - \frac{1}{2} \frac{2}{3} = 0.
 \end{equation}
\end{linenomath*}
Stąd $S_{x}$ jest dodatnie, co oznacza, że $x \in \Lambda$.

Niech teraz $x \in \Lambda$.
Wtedy dla każdego $A = A^{*} \in M_{3}$, $S_{x}$
spełnia nierówność Kadisona-Schwarza \eqref{eq:SchwarzInequality}:
    \begin{linenomath*}
 \begin{equation}
     \label{eq:KSineq}
        S(A)^{2} \leq S(A^{2}).
     \end{equation}
\end{linenomath*}
Niech $\vec{n} \in \mathbb{R}^{8}$;
ponieważ $\lambda(\frac{1}{\sqrt{3}}, \vec{n})$
jest macierzą samosprzężoną z \eqref{eq:KSineq}
oraz z faktu, że $S_{x}$ zachowuje ślad,
dostajemy:
    \begin{linenomath*}
 \begin{multline}
        \frac{1}{3} + || x \vec{n} ||_{2}^{2} = \text{Tr}\,
        \left ( S_{x}(\lambda(\frac{1}{\sqrt{3}}, \vec{n}) \right)^{2} \leq \\
   \leq \text{Tr}\, S \left( \lambda(\frac{1}{\sqrt{3}}, \vec{n})^{2} \right) =
        \text{Tr}\, \lambda(\frac{1}{\sqrt{3}}, \vec{n})^{2} =
        \frac{1}{3} + ||\vec{n}||_{2}^{2},
     \end{multline}
\end{linenomath*}
zatem $||x\vec{n}||_{2} \leq ||\vec{n}||_{2}$
dla każdego $\vec{n} \in \mathbb{R}^{8}$,
co daje w rezultacie: $x \in \overline{K}_{1}(\hat{0})$.
\end{proof}

Niech $P_{8} \in M_{8}(\mathbb{R})$ oznacza macierz
$P_{8} = \text{diag}(0,0,0,0,0,0,0,1)$
i tak dalej dla pozostałych zbiorów indeksów: np.
$P_{38} = \text{diag}(0,0,1,0,0,0,0,1)$.
Każda z macierzy
$P_{8}, P_{38}, P_{138}, P_{1238}, P_{13468}$ jest idempotentnym elementem
zbioru $\Lambda$.
Ponadto, w $\Lambda$ nie ma elementów idempotentnych rzędu 6 i 7
(p. tw. \ref{thm:Idempotents}),
a jedyny idempotent rzędu 8 to macierz jednostkowa $\mathbf{1}_{8}$.
Niech także $G_{3} = \text{Ad} \, \text{SU}(3) \subset \Lambda$ oznacza
grupę tych macierzy $g \in \Lambda$, dla których
$S_{g}$ jest automorfizmem: $S_{g}(A) = U A U^{*}$,
dla pewnej macierzy unitarnej $U \in \text{SU}(3)$.
Jasne jest, że $G_{3} \subset \text{SO}(8)$,
Zbiór tych $x$, dla których $S_{x}$ jest ekstremalnym
odwzorowaniem w zbiorze wszystkich odwzorowań na $M_{3}$
(niekoniecznie bistochastycznych) oznaczmy przez
$\text{Ext}_{0}(\Lambda)$,
a zbiór punktów ekstremalnych zwartego zbioru wypukłego
$\Lambda$ przez: $\text{Ext}(\Lambda)$.
Macierz $x \in \text{Ext}(\Lambda)$,
wtedy i tylko wtedy, gdy dla każdych
$y_{1}, y_{2} \in \Lambda$ i $0 \leq \lambda \leq 1$,
z tego, że $x = \lambda y_{1} + (1-\lambda) y_{2}$,
wynika: $y_{1} = y_{2} = x$.
Oczywiście, $\text{Ext}_{0}(\Lambda) \subset \text{Ext}(\Lambda)$.
Następny fakt wynika w prosty sposób z twierdzenia
\ref{thm:LambdaAndBalls}.

\begin{Theorem}
    \label{thm:oneHalfofOrthogonal}
    Niech $x \in \mathrm{Ext}(\Lambda)$.
Załóżmy, że $||x|| = \frac{1}{2}$;
wtedy $ x = \frac{1}{2} R$, dla $R \in \mathrm{O}(8)$.
\end{Theorem}
\begin{proof}
Niech $x \in \mathrm{Ext}(\Lambda)$ oraz $||x|| = \frac{1}{2}$.
Niech $x = R |x|$ będzie rozkładem biegunowym $x$,
    $R \in \text{O}(8)$, $|x| = \sqrt{x^{t} x}$.
Załóżmy, że $|x| \neq \frac{1}{2} \mathbf{1}_{8}$.
Niech $y_{1} = \frac{1}{2} \mathbf{1}_{8}$ i
    $y_{2} = 2 |x| - \frac{1}{2} \mathbf{1}_{8}$.
Wtedy $- \frac{1}{2} \mathbf{1}_{8} \leq y_{2} \leq \frac{1}{2} \mathbf{1}_{8}$,
a więc $|| y_{2} || \leq \frac{1}{2}$
i z twierdzenia \ref{thm:LambdaAndBalls},
mamy, że obie macierze, $R y_{1}, R y_{2} \in \Lambda$.
Stąd $x = \frac{1}{2} R y_{1} + \frac{1}{2} R y_{2}$,
i przez to $x$ nie może być punktem ekstremalnym, co oznacza sprzeczność.
Zatem, $x = \frac{1}{2} R$.
\end{proof}

Na mocy twierdzenia \ref{thm:LambdaAndBalls},
widzimy, że $x \in \text{Ext}(\Lambda)$ implikuje
$||x|| \geq \frac{1}{2}$.
Zatem $x = \frac{1}{2} R$, $R \in \mathrm{O}(8)$,
to jedyna możliwa postać elementów zbioru $\text{Ext}(\Lambda)$,
a co za tym idzie, również zbioru $\text{Ext}_{0}(\Lambda)$,
dla których $||x|| = \frac{1}{2}$.

Niech $x \in \Lambda$ oraz niech
$\langle x \rangle \subset \Lambda$ będzie półgrupą generowaną przez $x$:
$\langle x \rangle = \left \{ x^{k} \: | \: k \in \mathbb{N}, k \geq 1 \right \}$.
Oznaczmy przez $\overline{\langle x \rangle}$,
domknięcie $\langle x \rangle$ w zbiorze $M_{8}(\mathbb{R})$.
Dowód następującego faktu zawarty został w lemacie 3 w
pracy  \cite{schwarz1955hausdorff}.

\begin{Theorem}
    \label{prop:UniqeClusterPoint}
    Ponieważ $\Lambda$ jest zbiorem domkniętym, $\overline{\langle x \rangle} \subset \Lambda$.
    Zbiór $\overline{\langle x \rangle}$ zawiera jedyny element idempotentny,
    oznaczony symbolem $e_{x}$.
\end{Theorem}

\begin{Definition}
    Dla zbioru $\Lambda$, zdefiniujmy następujące podzbiory:
    \begin{enumerate}
       \item zbiór elementów idempotentnych w $\Lambda$:
            $\mathcal{E}(\Lambda) = \left \{ e \in \Lambda \: |  \: e^{2} = e \right \}$;
       \item zbiór uogólnionych elementów nilpotentnych:\\
            $\mathcal{N}(\Lambda) = \left \{ x \in \Lambda \: |  \: \lim \limits_{n \rightarrow \infty} x^{n} = \hat{0} \right \}$;
       \item grupa elementów odwracalnych:
            $\mathcal{G}(\Lambda) = \left \{ x \in \Lambda \: |  \: \exists \, y \in \Lambda, xy = yx = \mathbf{1}_{8} \right \}$.
    \end{enumerate}
   Dla elementu idempotentnego $e \in \mathcal{E}(\Lambda)$,
   zdefiniujmy następujące \mbox{podzbiory $\Lambda$}:
   \begin{enumerate}
        \item niech $H(e)$ będzie maksymalną podgrupą $\Lambda$
            zawierającą $e \in \mathcal{E}(\Lambda)$;
        \item $Q(e) = \left \{ x \in \Lambda \: | \: e_{x} = e \right \}$,
        gdzie $e_{x}$ jest jedynym idempotentnym elementem
        $\overline{\langle x \rangle}$.
   \end{enumerate}
\end{Definition}

\begin{Remark}
\label{rem:GOfLambda}
Na mocy znanego w teorii półgrup faktu, dla każdego elementu idempotentnego
istnieje dokładnie jedna maksymalna podgrupa zawierająca ten element.
W szczególności, w naszym przypadku półgrupy $\Lambda$,
$H(\hat{0}) = \left \{ \hat{0} \right \}$,
w oczywisty sposób:
$H(\mathbf{1}_{8}) = \mathcal{G}(\Lambda)$,
a także $H(P_{8}) = \left \{ P_{8} \right \}$.
Rzeczywiście,
jeśli $x = (x_{ij})_{i,j=1}^{8} \in H(P_{8})$
wówczas z faktu, że $x \in H(P_{8})$, wynika:
$x = x P_{8} = P_{8} x$
($x$ należy do maksymalnej grupy zawierającej $P_{8}$,
a macierz idempotentna $P_{8}$ jest elementem jednostkowym tej grupy).
Stąd $x = P_{8} \, x P_{8} = x_{88} P_{8}$.
Ponieważ istnieje ciąg liczb naturalnych  $n_{k}$, $k=1,2,3,\ldots$,
taki że $x^{n_{k}} \overset{k}{\rightarrow} P_{8}$,
więc
$|x_{88}| = 1$. Jednakże $- P_{8} \notin \Lambda$,
a stąd $x = P_{8}$.
\end{Remark}

Będziemy powoływać się niejednokrotnie na następujące twierdzenie,
udowodnione w ogólności w pracy
\cite{schwarz1955hausdorff}:
\begin{Theorem}
    \label{prop:SchwarzSemigroups}
    Niech $e \in  \mathcal{E}(\Lambda)$.
    Wówczas $H(e) = e \, Q(e) = Q(e) \, e$.
\end{Theorem}
\begin{proof}
    Zauważmy, że półgrupa $\Lambda$ spełnia założenia Twierdzenia 8
    w pracy \cite{schwarz1955hausdorff}.
\end{proof}

\begin{Remark}
    \label{rem:Qof1}
Prawdą jest, że $Q(\hat{0}) = \mathcal{N}(\Lambda)$.
Rzeczywiście, niech $x \in Q(\hat{0})$;
wówczas na mocy definicji,
istnieje ciąg liczb naturalnych $n_{k}$, taki że
$x^{n_{k}} \overset{k}{\rightarrow} \hat{0}$.
Dla pewnego ustalonego $k$ i dla każdego $l \geq n_{k}$,
skoro $||x|| \leq 1 $
(por. tw. \ref{thm:LambdaAndBalls}),
mamy
$||x^{l}|| \leq ||x^{l - n_{k}}|| \, ||x^{n_{k}}|| \leq ||x^{n_{k}}||$.
Zatem $x^{l} \overset{l}{\rightarrow} \hat{0}$
i $Q(\hat{0}) \subset \mathcal{N}(\Lambda)$.
Przeciwne zawieranie jest oczywiste.
Co więcej, ponieważ mamy $H(\mathbf{1}_{8}) = \mathcal{G}(\Lambda)$,
z twierdzenia \ref{prop:SchwarzSemigroups},
$H(\mathbf{1}_{8}) = Q(\mathbf{1}_{8}) \cdot \mathbf{1}_{8}$,
a wiec $Q(\mathbf{1}_{8}) = \mathcal{G}(\Lambda)$.
\end{Remark}

Naszym zadaniem jest analiza rodziny zbiorów
$\left \{ Q(e), e \in \mathcal{E}(\Lambda) \right \}$,
a w szczególności pokazanie, w których spośród tych zbiorów należy szukać
elementów ekstremalnych z
$\text{Ext}_{0}(\Lambda)$.
W tym celu przedstawmy następujący ciąg stwierdzeń.

\begin{Theorem}
    Dla $e_{1}, e_{2} \in \mathcal{E}(\Lambda)$,
    $e_{1} \neq e_{2}$,
    zbiory $Q(e_{1})$ i $Q(e_{2})$ są rozłączne.
Ponadto,
    $\Lambda = \! \bigcup \limits_{e \in \mathcal{E}(\Lambda)} \! Q(e)$.
\end{Theorem}
\begin{proof}
Jeśli $e_{1} \neq e_{2}$, na mocy twierdzenia \ref{prop:UniqeClusterPoint}
zbiory $Q(e_{1})$ i $Q(e_{2})$ muszą być rozłączne.
Dla $x \in \Lambda$ mamy, że $x \in Q(e_{x}) \subset \Lambda$,
co dowodzi tezy.
\end{proof}

Należy dodatkowo zaznaczyć, że dla $g \in \mathcal{G}(\Lambda)$,
ponieważ $||g|| \leq 1$ (por. tw. \ref{thm:LambdaAndBalls}),
i $g^{-1}  \in \Lambda$, więc $|| g || = 1$.
Zatem $g$ jest macierzą ortogonalną.
W szczególności,
dla każdego $e \in \mathcal{E}(\Lambda)$,
$g e g^{t} \in \mathcal{E}(\Lambda)$.

\begin{Lemma}
    \label{lem:equivClassesOfQe}
    Niech $e \in \mathcal{E}(\Lambda)$ i $g \in \mathcal{G}(\Lambda)$.
    Wówczas $Q(g e g^{t}) = g Q(e) g^{t} =
    \left \{ g x g^{t} \: | \: x \in Q(e) \right \}$.
\end{Lemma}
\begin{proof}
    Jasne jest, że $g \langle x \rangle g^{t} = \langle g x g^{t} \rangle$.
Ponieważ odwzorowanie $x \mapsto g x g^{t}$ jest homeomorfizmem,
więc $g \overline{\langle x \rangle} g^{t} = \overline{\langle g x g^{t} \rangle}$.
Niech $x \in Q(g e g^{t})$.
Wtedy $g e g^{t} \in \overline{\langle x \rangle}$.
Stąd $e \in g^{t} \overline{\langle x \rangle} g = \overline{\langle g^{t} x g \rangle}$,
co oznacza, że $g^{t} x g \in Q(e)$, czyli
    $x \in g Q(e) g^{t}$.
To potwierdza, że
    $Q(g e g^{t}) \subset g Q(e) g^{t}$.
Zawieranie w drugą stronę dowodzi się w sposób analogiczny.
\end{proof}

W dalszej kolejności spróbujmy opisać strukturę elementów idempotentnych w $\Lambda$.
Dla $e \in \mathcal{E}(\Lambda)$,
niech $\mathcal{E}_{G_{3}}(e) = \left \{ g e g^{t} \: | \: g \in G_{3} \right \}$.
Z początku, w następnym lemacie, przypomnijmy znany fakt,
mówiący, że idempotenty operator kontraktywny na przestrzeni Hilberta
jest projektorem ortogonalnym
(por. np. zadanie 5.3.14 w książce
Y.\,Abramovicha i  C.\,Aliprantisa,
\emph{Problems in operator theory}
\cite{Abramovich2002}).
\begin{Lemma}
    \label{lem:eIsProj}
    Niech $e \in \mathcal{E}(\Lambda)$, wówczas $e^{t} = e$ i stąd,
    $e$ jest projektorem ortogonalnym w $M_{8}(\mathbb{R})$.
\end{Lemma}
\begin{proof}
    Ponieważ $e^{2} = e$, więc $||e|| = ||e^{2}|| \leq ||e||^{2}$,
    zatem $||e|| \geq 1$.
    Ponieważ $e \in \Lambda$, na mocy twierdzenia \ref{thm:LambdaAndBalls},
    $||e|| =1$.
Załóżmy, że $\vec{n} \in \ker e$ oraz $\vec{m} \in \mathcal{R}(e)$,
gdzie $\mathcal{R}(e)$ oznacza obraz operatora $e$.
Niech $\alpha \in \mathbb{R}$.
Wówczas
\begin{linenomath*}
 \begin{equation}
    || \alpha \vec{m} ||^{2} = || P (\vec{n} + \alpha \vec{m}) ||^{2} \leq
    || \vec{n} + \alpha \vec{m}||^{2} \leq ||\vec{n}||^{2} +
        2 \alpha \langle \vec{n}, \vec{m} \rangle + || \alpha \vec{m}||^{2},
 \end{equation}
\end{linenomath*}
tzn. $||\vec{n}||^{2} + 2 \alpha  \langle \vec{n}, \vec{m} \rangle \geq 0$,
dla dowolnego $\alpha \in \mathbb{R}$.
To z kolei oznacza, że
$\langle \vec{n}, \vec{m} \rangle = 0$,
dla każdego  $\vec{n} \in \ker e$ i $\vec{m} \in \mathcal{R}(e)$,
tzn.
$\ker e \perp \mathcal{R}(e)$, co dowodzi, że $e = e^{t}$.
\end{proof}

\begin{Theorem}
\label{prop:htranspose}
Niech $e \in \mathcal{E}(\Lambda)$ i $h \in H(e)$.
Wtedy $h^{t} \in H(e)$, a także $h^{t} h = h h^{t} = e$.
\end{Theorem}
\begin{proof}
    Z twierdzenia \ref{prop:SchwarzSemigroups} mamy:
    $H(e) = Q(e) e = e \, Q(e)$.
Jeśli $x \in Q(e)$, to $x^{t} \in Q(e)$,
na mocy lematu \ref{lem:eIsProj}.
Stąd, $h^{t} \in H(e)$.
Ponieważ $e$ jest projektorem ortogonalnym, $e \leq \mathbf{1}_{8}$;
co więcej $h^{t} h \leq \mathbf{1}_{8}$, gdyż $h \in \Lambda$.
A wiec $\hat{0} \leq (h^{t} h)^{k} \leq h^{t} h = e h^{t} h e \leq e$,
dla każdego $k \in \mathbb{N}$.
Jeśli $n_{k}$ jest ciągiem liczb naturalnych, takim że
    $(h^{t} h)^{n_{k}} \overset{k}{\rightarrow} e$,
otrzymujemy: $h^{t} h = e$.
Na mocy takiego samego rozumowania, mamy także: $h h^{t} = e$.
\end{proof}

Przypomnijmy, że dla  bistochastycznego odwzorowania $S$
przez $K_{S}$ oznaczamy stabilną podprzestrzeń zdefiniowaną przez
\begin{linenomath*}
 \begin{equation}
    K_{S} = \left \{ x \in M_{3} \:|\:
            \forall k \in \mathbb{N} \,\,
            || S^{k} x ||_{HS} = || S^{*k} x ||_{HS} =  ||x||_{HS}
    \right \},
 \end{equation}
\end{linenomath*}
gdzie $S^{*}$ oznacza odwzorowanie sprzężone.
Fakt, że $H(\mathbf{1}_{8}) = \mathcal{G}(\Lambda)$
można uogólnić do następującego twierdzenia.
\begin{Theorem}
    Niech $e \in \mathcal{E}(\Lambda)$ i
    $K_{S_{e}}$ będzie stabilną podprzestrzenią dla odwzorowania $S_{e}$.
    Wówczas $K_{S_{e}}$ jest algebrą Jordana oraz
    $H(e)$ jest izomorficzny z grupą $\text{Aut}_{J} \, K_{S_{e}}$
    automorfizmów Jordana algebry $K_{S_{e}}$.
\end{Theorem}

\begin{proof}
Ponieważ $e$ jest macierzą idempotentną, na mocy
wniosku \ref{cor:KisJordanAlgebra},
przestrzeń $K_{S_{e}} = S_{e}(M_{3})$ jest algebrą Jordana.
Odwzorowanie $S_{e}$ jest w rzeczywistości
projektorem o normie jednostkowej (\emph{conditional expectation})
na $K_{S_{e}}$.
Niech $h \in H(e)$.
Z twierdzenia \ref{prop:htranspose}, dla dowolnego $k\in \mathbb{N}$,
$S_{h}^{* k} S_{h}^{k} = S_{h^{t}}^{k} S_{h}^{k} =  S_{(h^{t})^{k} h^{k}} = S_{e}$,
i podobnie dla $S_{h}^{k} S_{h}^{* k}= S_{e}$.
Zatem, stabilna algebra $K_{S_{h}} = K_{S_{e}}$.
Znów na mocy wniosku \ref{cor:KisJordanAlgebra},
$\varphi_{h} = S_{h} \big |_{K_{S_{e}}}$
jest automorfizmem Jordana algebry $K_{S_{e}}$.

Z drugiej strony, jeżeli $\varphi$ jest dowolnym automorfizmem Jordana na
$K_{S_{e}} \subset M_{3}$,
można rozszerzyć go do odwzorowania bistochastycznego $S_{h}$ na $M_{3}$,
dla pewnego $h \in \Lambda$,
kładąc $S_{h} = \varphi \circ S_{e}$.
Wtedy
    $S_{h} S_{e} = S_{h} = S_{e} S_{h}$,
ponieważ $S_{e}$ działa na $K_{S_{e}}$ jak odwzorowanie identycznościowe.
Stąd, $he = eh = h$.
Dodatkowo, ponieważ $\varphi$ jest odwracalne na $K_{S_{e}}$,
rozszerzając $\varphi^{-1}$ do kolejnego odwzorowania bistochastycznego
    $S_{h'}$,  $h' \in \Lambda$,
pokazujemy, że $h' \in H(e)$ i $h' h = h h' = e$;
co dowodzi, że $h \in H(e)$.
\end{proof}
\begin{Remark}
Z powyższego twierdzenia, ponieważ $\mathcal{G}(\Lambda) = H(\mathbf{1}_{8})$,
wnioskujemy, że grupa $\mathcal{G}(\Lambda)$ zawiera te macierze,
które reprezentują automorfizmy Jordana na $M_{3}$,
tzn. dla każdego $g \in \mathcal{G}(\Lambda)$,
istnieje macierz unitarna $U \in \text{SU}(3)$, taka że
albo $S_{g}(A) = U A U^{*}$, albo $S_{g}(A) = U A^{t} U^{*}$
dla każdego $A \in M_{3}$.
Jasne jest, że $G_{3} \subset \mathcal{G}(\Lambda)$.
Elementy $G_{3}$ reprezentują dokładnie te izomorfizmy Jordana,
które są jednocześnie zwykłymi izomorfizmami;
dla każdego $g \in \mathcal{G}(\Lambda) \backslash G_{3}$ istnieje
$g' \in G_{3}$, takie że $g = g' \tau$,
gdzie $\tau \in \mathcal{G}(\Lambda)$ jest wybrane tak, aby
$S_{\tau}(A) = A^{t}$ dla każdego $A \in M_{3}$,
czyli
$\tau = P_{13468} - P_{257}$.
\end{Remark}

\begin{Theorem}
\label{thm:Idempotents}
Zbiór $\mathcal{E}(\Lambda)$
jest sumą mnogościową siedmiu rozłącznych zbiorów:
    \begin{linenomath*}
 \begin{equation}
        \mathcal{E}(\Lambda) = \bigcup \limits_{e_{0} \in J} \mathcal{E}_{G_{3}}(e_{0}),
     \end{equation}
\end{linenomath*}
    gdzie $J = \left \{ \hat{0}, P_{8}, P_{38}, P_{138}, P_{1238}, P_{13468}, \mathbf{1}_{8} \right \}$.
\end{Theorem}
\begin{proof}
Jeżeli $e \in \mathcal{E}(\Lambda)$, to tak, jak powyżej, $S_{e}$
jest projektorem o normie jednostkowej na algebrę Jordana $K_{S_{e}}$.
Tak jak na str. \pageref{page:allpossibleJalg},
na mocy twierdzeń 5.3.8 i 6.2.3 \cite{Hanche1984},
wszystkie algebry Jordana w $M_{3}$ są izomorficzne (unitarnie równoważne) z
jedną z następujących:
    $\mathbb{C}\mathbf{1}$,
    $\mathbb{C} E_{12} \oplus \mathbb{C} E_{3}$,
    $\mathbb{C} E_{1} \oplus \mathbb{C} E_{2} \oplus \mathbb{C} E_{3}$,
    $M_{2}^{s} \oplus \mathbb{C} E_{3}$,
    $M_{2} \oplus \mathbb{C} E_{3}$,
    $M_{3}^{s}$,
    i
    samą algebrą $M_{3}$;
gdzie $M_{k}^{s}$ jest algebrą Jordana algebra macierzy symetrycznych rozmiaru
$k$: $M_{k}^{s} = \{ A \in M_{k}: A = A^{t} \}$;
    $E_{i}, i = 1,2,3$, to macierze diagonalne z 1 na \emph{i}-tym miejscu na
    przekątnej i 0 na pozostałych miejscach oraz $E_{12} = E_{1} + E_{2}$.
Zatem, istnieje $g \in G_{3}$, takie że
    $e = g e_{0} g ^{t}$, a $e_{0}$ jest projektorem ortogonalnym
    reprezentującym odwzorowanie rzutowe na dokładnie jedną z algebr Jordana
    wymienionych powyżej.
Łatwo sprawdzić, że wówczas $e_{0} \in J$ oraz
$\text{dim} \, e_{0} + 1$ jest równe wymiarowi odpowiedniej algebry Jordana.
Zatem $e \in \mathcal{E}_{G_{3}}(e_{0})$.
\end{proof}

\begin{Corollary}
\label{cor:q}
    Dla $e \in \mathcal{E}(\Lambda)$, z uwagi na to, że $e$ jest projektorem,
    $\text{dim} \, e
    \in \left \{ 0,1,2,3,4,5,8 \right \}$.
\end{Corollary}

W następnej kolejności dowiedziemy istnienia użytecznego rozkładu elementów
zbioru $Q(e)$.

\begin{Lemma}
\label{lem:decomposition}
    Niech $e \in \mathcal{E}(\Lambda)$.
    Macierz $x$ należy do $Q(e)$, wtedy i tylko wtedy, gdy
    $x = h + y$, gdzie
    $h \in H(e)$, $H(e) y = y H(e) = \hat{0}$ oraz
    $\lim_{k \rightarrow \infty} y^{k} = \hat{0}$.
    Rozkład ten jest jedyny.
\end{Lemma}

\begin{proof}
    Załóżmy, że $x = h + y \in \Lambda$, $h \in H(e)$, $y^{k}  \overset{k}{\rightarrow}  \hat{0}$
    oraz $hy = yh = \hat{0}$.
Ponieważ $h \in H(e)$, istnieje ciąg $n_{k} \in \mathbb{N}$ dla $k =1,2,3,\ldots$,
taki że $h^{n_{k}} \overset{k}{\rightarrow} e$.
Wówczas $x^{n_{k}}  = h^{n_{k}} + y^{n_{k}}  \overset{k}{\rightarrow}  e$, tzn.
    $x \in Q(e)$.
Z drugiej strony
załóżmy, że $x \in Q(e) \subset \Lambda$.
Mamy  $x = e x e + e^{\perp} x e + e x e^{\perp} + e^{\perp} x ^{\perp}$,
gdzie $e^{\perp} = \mathbf{1}_{8} - e$.
Na mocy tw. \ref{prop:SchwarzSemigroups},
obie macierze $x e, x e \in H(e)$.
Zatem, $e x e = x e = e x$, co oznacza, że $e^{\perp} x e = \hat{0}$
oraz $e x e^{\perp} = \hat{0}$.
Niech $h = e x e$, and $y = e^{\perp} x e^{\perp}$.
Wiemy, że $x = h + y$ i $h y = y h = \hat{0}$.
Wówczas $h^{k} = e x^{k} e$, dla $k \in \mathbb{N}$,
co implikuje: $h \in Q(e)$.
Ponieważ
    $h =  h e = e h$,
więc również
    $h \in H(e)$, na mocy twierdzenia \ref{prop:SchwarzSemigroups}.
Dla $h' \in H(e)$, mamy
    $h' y = h' e y = \hat{0} = y e h' = yh'$,
tzn. $H(e) y = y H (e) = \hat{0}$.
Istnieje ciąg $n_{k}$ liczb naturalnych, taki że
    $x^{n_{k}}  \overset{k}{\rightarrow}  e$.
Stąd
    $h^{n_{k}} = e x^{n_{k}} e  \overset{k}{\rightarrow} e^{3}  = e$.
W konsekwencji dostajemy $y^{n_{k}}  \overset{k}{\rightarrow}  \hat{0}$,
co wystarcza, aby powiedzieć, że $y^{k} \overset{k}{\rightarrow} \hat{0}$.
Dalej załóżmy, że $x = h + y = h_{2} + y_{2}$,
    gdzie $h_{2} \in H(e)$, a $y_{2}$ jest takie, że
    $h_{2} y_{2} = y_{2} h_{2} = \hat{0}$.
Dla pewnego ciągu liczb naturalnych $m_{k}$, mamy
    $e y_{2} = \lim_{k} h_{2}^{m_{k}} y_{2} = \hat{0}$.
Wtedy $h = e x = e h_{2} + e y_{2} = h_{2}$, a więc również $y = y_{2}$.
\end{proof}

Dla $x \in Q(e)$, ponieważ rozkład opisany powyżej jest jedyny,
oznaczmy przez $h_{x}$ i $y_{x}$ macierze, dla których
$x = h_{x} + y_{x}$,
$h_{x} \in H(e)$, $H(e) y_{x} = y_{x} H(e) = \hat{0}$ oraz
$y_{x}^{k} \overset{k}{\rightarrow} \hat{0}$.
Powyższy lemat uzasadnia następującą definicję.

\begin{Definition}
    Niech $e \in \mathcal{E}(\Lambda)$;
    zdefiniujmy:
    $Q_{0}(e) = \left \{ x \in Q(e) \: | \: || y_{x} || < 1 \right \}$.
    Niech także $Q_{i}(e)$ będzie zbiorem składającym się z tych
    $x = h_{x} + y_{x} \in Q(e)$,
    dla których największa wartość singularna $y_{x}$ jest równa 1 i
    posiada wielokrotność $i$.
\end{Definition}

Jasne jest, że
$Q(e) = \bigcup \left \{ Q_{i}(e), i=1,2,\ldots,8 \right \} \cup Q_{0}(e)$
oraz że te zbiory są rozłączne (być może niektóre z nich są również puste).

\begin{Theorem}
Niech $e \in \mathcal{E}(\Lambda)$.
Jeżeli $\mathrm{dim} \, e \in \left \{ 5,8 \right \}$,
wówczas
$Q(e) = Q_{0}(e)$.
Jeżeli zaś $\mathrm{dim} \, e \leq 4$,
to
$Q_{i}(e) = \emptyset$ dla $i \geq 5 - \mathrm{dim} \, e$.
\end{Theorem}
\begin{proof}
    Gdy $\mathrm{dim} \, e = 8$,
    wówczas $e = \mathbf{1}_{8}$ i na mocy uwagi \ref{rem:Qof1},
    $Q(\mathbf{1}_{8}) = H(\mathbf{1}_{8}) = \mathcal{G}(\Lambda)$,
    w oczywisty sposób $Q(\mathbf{1}_{8}) = Q_{0}(\mathbf{1}_{8})$.
    Załóżmy, że $e \neq \mathbf{1}_{8}$
oraz $x = h_{x} + y_{x} \in Q(e)$.
Wtedy $x^{t} x = h_{x}^{t} h_{x} + y_{x}^{t} y_{x} = e + y_{x}^{t} y_{x}$,
z twierdzenia \ref{prop:htranspose};
    $y_{x} e = e y_{x} = \hat{0}$,
a ponieważ dla każdego $k \in \mathbb{N}$, $(x^{t} x)^{k} \in \Lambda$,
mamy, że $e + p = \lim_{k} x^{t} x \in \Lambda$,
gdzie $p$ jest projektorem ortogonalnym na podprzestrzeń rozpiętą przez wektory
własne $y_{x}^{t} y_{x}$, odpowiadające wartości własnej 1.
Oczywiście, $e p = p e = \hat{0}$, więc
macierz $e + p \in \mathcal{E}(\Lambda)$.
Musi być tak, że $e + p \neq \mathbf{1}_{8}$,
w przeciwnym wypadku $x^{t} x = \mathbf{1}_{8}$, a ponieważ
    $x \in \mathcal{G}(\Lambda) = Q(\mathbf{1}_{8})$, dostajemy sprzeczność.
Jeżeli $\mathrm{dim}\,e = 5$, to $p = \hat{0}$,
gdyż z twierdzenia \ref{thm:Idempotents}
nie istnieją elementy idempotentne w  $\Lambda$
rzędu $6$ lub $7$.
Stąd $||y|| < 1$ i $x \in Q_{0}(e)$.
Dzięki praktycznie tej samej argumentacji, dla $\textrm{dim}\, e \leq 4$,
nie jest możliwe, aby $\mathrm{dim} \, p  + \mathrm{dim}\, e \geq 5$,
a więc $Q_{i}(e) = \emptyset$, dla $i \geq 5 - \mathrm{dim} \, e$,
ponieważ z definicji, $i = \textrm{dim}\,p$.
\end{proof}

Aby uprościć notację w dalszej części wywodu,
wprowadźmy ciąg elementów z $\mathcal{E}(\Lambda)$:
$p_{0} = \hat{0}, p_{1} = P_{8}$, $p_{2} = P_{38}$,
$p_{3} = P_{138}$, $p_{4} = P_{1238}$, $p_{5} = P_{13468}$.
\begin{Theorem}
\label{thm:LowerIndices}
    Niech $i,j$ będą liczbami całkowitymi, takimi że
    $1 \leq i \leq 4$, $0 \leq j \leq 4$ oraz $i+j \leq 5$.
Jeśli $x \in Q_{i}(p_{j})$,
wówczas istnieją $g_{1}, g_{2} \in G_{3}$ i $z \in Q_{0}(p_{i+j})$,
dla których $x = g_{1} z g_{2}$.
\end{Theorem}
\begin{proof}
    Niech $x \in Q_{i}(p_{j})$ i
    $x = h_{x} + y_{x}$, jak wyżej.
    Ponieważ taki rozkład jest jedyny, możemy napisać jak w dowodzie
    lematu \ref{lem:decomposition}:
    $h_{x} = p_{j} x p_{j}$ oraz $y_{x} = p_{j}^{\perp} x p_{j}^{\perp}$,
    gdzie $p_{j}^{\perp} = \mathbf{1}_{8} - p_{j}$.
    Ponieważ $p_{j} = p_{j}^{t}$, więc
    $h y^{t} = y h^{t} = \hat{0}$ i dlatego
    $x x^{t}  = h h^{t} + y y^{t} = p_{j} + y y^{t}$,
    na mocy \mbox{twierdzenia \ref{prop:htranspose}}.
    Niech $R_{1} (p_{i} + y_{0}) R_{2}$ będzie rozkładem $y$ na wartości
    singularne, tzn. $R_{1}, R_{2} \in \text{O}(8)$ są macierzami ortogonalnymi,
    $y_{0}$ jest macierzą diagonalną z jedynymi możliwymi niezerowymi elementami
    $s_{1}, s_{2}, \ldots, s_{8-i}$, takimi że
    $1 > s_{1} \geq s_{2} \geq \ldots \geq s_{8-i} \geq 0$
    i $p_{i} y_{0} = y_{0} p_{i} = \hat{0}$.
    Wówczas dla $k \in \mathbb{N}$,
    $(x x^{t})^{k} = p_{j} + R_{1}( p_{i} + (y_{0} y_{0}^{t})^{k}) R_{1}^{t} \in \Lambda$,
    i ponieważ $\Lambda$ jest zbiorem domkniętym:
    $e_{1} = p_{j} + R_{1} p_{i} R_{1}^{t} = \lim_{k} (x x^{t})^{k} \in \Lambda$.
    Ponieważ $h y^{t} = y h^{t} = \hat{0}$,
    mamy $p_{j} R_{1} p_{i} R_{1}^{t} = R_{1} p_{i} R_{1}^{t} p_{j} = \hat{0}$.
    Stąd wynika, że $e_{1}$ jest macierzą idempotentną i
    $\text{rank} \, e_{1} = i+j$.
    Z twierdzenia \ref{thm:Idempotents}, istnieje $g_{1} \in G_{3}$, takie że
    $e_{1} = g_{1} p_{i+j} g_{1}^{t}$.
    Podobna argumentacja,
    tym razem zastosowana do $x^{t} x$, pokazuje,
    że istnieje także $g_{2} \in G_{3}$,
    takie że macierz idempotentną
    $e_{2} = p_{j} + R_{2}^{t} p_{i} R_{2} \in \Lambda$
    można zapisać jako $e_{2} = g_{2}^{t} p_{i+j} g_{2}$.
    Niech $z = g_{1}^{t} x g_{2}^{t}$.
    Pozostaje pokazać, że $z \in Q_{0}(p_{i+j})$.
    Jasne jest, że $z \in \Lambda$.
    Łatwo można również sprawdzić, że $p_{i+j} z = z p_{i+j}$,
    a stąd $z = h_{z} + y_{z}$, gdzie $h_{z} = p_{i+j} z p_{i+j}$
    oraz $y_{z} = p_{i+j}^{\perp} z p_{i+j}^{\perp}$,
    $p_{i+j}^{\perp} = \mathbf{1}_{8} - p_{i+j}$.
    Ponieważ $h_{z} = p_{i+j} h_{z} = h_{z} p_{i+j}$,
    $h_{z} \in H(p_{i+j})$.
    Oczywiście, $h y_{z} = y_{z} h = \hat{0}$, dla każdego $h \in H(p_{i+j})$.
    Dodatkowo mamy, że
    $y_{z} = g_{1}^{t}R_{1} y_{0} R_{2} g_{2}^{t}$, więc
    $||y_{z}^{k}|| \leq ||y_{0}||^{k} = s_{1}^{k} \overset{k}{\rightarrow} \hat{0}$.
    Zatem, z lematu \ref{lem:decomposition}, $z \in Q(p_{i+j})$.
    Ponieważ $||y_{z}|| < 1$, dostajemy, że
    $z \in Q_{0}(p_{i+j})$, co kończy dowód.
\end{proof}

Główny wynik tej części wywodu można sformułować w postaci następującego wniosku.
\begin{Corollary}
\label{cor:q0}
    Załóżmy, że $x \in \text{Ext}_{0}(\Lambda)$ oraz $S_{x}$ nie jest
    automorfizmem Jordana.
    Wówczas istnieją $g_{1}, g_{2} \in G_{3}$,
    takie że $g_{1}^{t} x g_{2}^{t} \in Q_{0}(\hat{0}) \cup Q_{0}(P_{8})$.
    Innymi słowy, albo $||x|| < 1$,
    albo
    $x = g_{1} (P_{8} + y) g_{2}$,  $y P_{8} = P_{8} y = \hat{0}$
    i $|| y || < 1$.
\end{Corollary}
\begin{proof}
Niech $x \in  \text{Ext}_{0}(\Lambda)$.
    Z lematu \ref{lem:equivClassesOfQe} i twierdzenia \ref{thm:Idempotents},
    istnieje $g \in G_{3}$, takie że
    $g^{t} x g \in Q(e_{0})$ i $e_{0} \in J$.
    Ponieważ $S_{x}$ jest ekstremalnym odwzorowaniem dodatnim, więc na mocy
    twierdzenia \ref{thm:ExposedMaps},
    $e_{0} \in \left \{ \hat{0}, P_{8}, \mathbf{1}_{8} \right \}$.
    Z uwagi \ref{rem:Qof1}, $x \notin Q(\mathbf{1}_{8})$, a zatem
    $z = g^{t} x g \in Q(\hat{0}) \cup Q(P_{8})$,
    i $z \in \text{Ext}_{0}(\Lambda)$
    (por. lemat 3.1.2 \cite{Stormer2013}).
    Załóżmy, że $z \in Q(P_{8})$.
    Z twierdzenia \ref{thm:LowerIndices},
    ponieważ $z \in \text{Ext}_{0}(\Lambda)$,
    $z \notin Q_{i}(P_{8})$
    dla $i \geq 1$.
    A zatem $z \in Q_{0}(P_{8})$ i połóżmy $g_{1} = g$ i $g_{2} = g^{t}$.
    Wówczas $x = g_{1} z g_{2}$,
    a z lematu \ref{lem:decomposition}, ponieważ
    $H(P_{8}) = \left \{ P_{8} \right \}$
    (por. uwaga \ref{rem:GOfLambda}),
    $z = P_{8} + y$,  $y P_{8} = P_{8} y = \hat{0}$ i $||y|| < 1$.
    Z drugiej strony, jeśli $z \in Q(\hat{0})$,
    wtedy na mocy tego samego rozumowania albo $z \in Q_{0}(\hat{0})$
    i $||z|| < 1$, ponieważ $H(\hat{0}) = \left \{ \hat{0} \right \}$; albo
    $z \notin Q_{0}(\hat{0})$ i wówczas istnieją $g_{01}, g_{02} \in G_{3}$,
    takie że $g_{01} z g_{02} \in Q_{0}(P_{8})$.
    Wtedy połóżmy $g_{1} = g g_{01}^{t}$ i $g_{2} = g_{02}^{t} g^{t}$,
    co kończy dowód.
\end{proof}

\label{com:onlyOneExample}
Podsumujmy wyniki przedstawione powyżej zauważając, że pozwalają nam one
na sprecyzowanie problemu znalezienia ekstremalnych odwzorowań bistochastycznych
na $M_{3}(\mathbb{C})$ poprzez skupienie uwagi na trzech szczególnych
przypadkach.
Po pierwsze, są to izomorfizmy Jordana, reprezentowane przez macierze z
$Q(\mathbf{1}_{8}) = \mathcal{G}(\Lambda)$.
Następnie, odwzorowania, które możemy nazwać \emph{silnie ergodycznymi},
należące do klasy reprezentowanej przez elementy zbioru
$\left \{ x \in Q_{0}(\hat{0}) \:|\: ||x|| = \frac{1}{2} \right \}$.
Jednym z przykładów takich odwzorowań jest znane przekształcenie podane
po raz pierwszy przez M.-D.\,Choi \cite{choi1977extremal}.
Uogólnione odwzorowanie tego typu ma postać
\begin{linenomath*}
 \begin{multline}\label{eq:choi}
\Phi[a,b,c](X)=\\
\frac{1}{2}
\begin{pmatrix}
ax_{11}+bx_{22}+cx_{33} & -x_{12} & -x_{13} \\
-x_{21} & cx_{11}+ax_{22}+bx_{33} & -x_{23} \\
-x_{31} & -x_{32} & bx_{11}+cx_{22}+ax_{33}
\end{pmatrix},
 \end{multline}
\end{linenomath*}
gdzie $X = (x_{ij})_{i,j = 1}^{3}$,
jeżeli wprowadzimy parametryzację:
\begin{linenomath*}
 \begin{equation}
 a(t)=\dfrac{(1-t)^2}{1-t+t^2},\quad b(t)=\dfrac{t^2}{1-t+t^2},\quad c(t)=\dfrac 1{1-t+t^2},
 \end{equation}
\end{linenomath*}
$0 \leq t < 1$,
wówczas $\Phi[a(t), b(t), c(t)]$ jest odwzorowaniem bistochastycznym, ekstremalnym
w zbiorze wszystkich odwzorowań dodatnich na $M_{3}$.
Dla $t = 0$, otrzymujemy przekształcenie zaproponowane oryginalnie przez Choi,
a dla $t = 1$ -- przekształcenie kompletnie dodatnie
(por. K.-C.\,Ha, S.-H.\,Kye \cite{ha2011entanglement}).
Rodzina macierzy $a_{t}$, dla której
$S_{a_{t}} = \Phi[a(t), b(t), c(t)]$ jest dana przez
\begin{linenomath*}
 \begin{equation}
 a_{t} = \begin{pmatrix}
     - \frac{1}{2} & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
     0 & - \frac{1}{2} & 0 & 0 & 0 & 0 & 0 & 0 \\
     0 & 0 & \frac{1 - 4t + t^{2}}{4(1 - t + t^{2})} & 0 & 0 & 0 & 0 & - \frac{\sqrt{3}(1 - 4t + t^{2})}{4(1 - t + t^{2})} \\
     0 & 0 & 0 & - \frac{1}{2} & 0 & 0 & 0 & 0 \\
     0 & 0 & 0 & 0 & - \frac{1}{2} & 0 & 0 & 0 \\
     0 & 0 & 0 & 0 & 0 & - \frac{1}{2} & 0 & 0 \\
     0 & 0 & 0 & 0 & 0 & 0 & - \frac{1}{2} & 0 \\
     0 & 0 & \frac{\sqrt{3}(1 - 4t + t^{2})}{4(1 - t + t^{2})} & 0 & 0 & 0 & 0 & \frac{1 - 4t + t^{2}}{4(1 - t + t^{2})} \\
 \end{pmatrix}.
 \end{equation}
\end{linenomath*}
Warto zaznaczyć, że
$a_{t} = \frac{1}{2} R_{t}$, gdzie $R_{t} \in \text{O}(8)$,
tak jak w twierdzeniu \ref{thm:oneHalfofOrthogonal}.

Ostatnią grupą są odwzorowania reprezentowane przez elementy z $Q_{0}(P_{8})$,
a jednym spośród nich jest odwzorowanie zadane przez równanie
\eqref{eq:DefinitionOfS} z poprzedniego podrozdziału.
Macierz
$x \in \Lambda$ w tym przypadku ma postać diagonalną:
\begin{linenomath*}
 \begin{equation}
  x =
 \text{diag}(0,0,0,
 \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, \frac{-1}{\sqrt{2}},
 1).
 \end{equation}
\end{linenomath*}

Powiemy, że dwie macierze $x, y \in \Lambda$ są równoważne,
wtedy i tylko wtedy, gdy istnieją $g_{1}, g_{2} \in G_{3}$
takie, że $x = g_{1} y g_{2}$.
Wówczas zadanie znalezienia elementów zbioru $\text{Ext}_{0}(\Lambda)$,
z dokładnością do tej właśnie relacji równoważności,
sprowadza się po pierwsze do znalezienia wszystkich $R \in \mathrm{O}(8)$,
takich że
 $\frac{1}{2}R \in \text{Ext}_{0}(\Lambda)$;
po drugie do rozstrzygnięcia, które macierze  $x \in \Lambda$,
 takie że $\frac{1}{2} < ||x|| < 1$, należą do $\text{Ext}_{0}(\Lambda)$.
Po trzecie zaś do  znalezienia wszystkich $y$, dla których
 $y P_{8} = P_{8} y = \hat{0}$, $||y|| < 1$
 oraz $P_{8} + y \in \text{Ext}_0(\Lambda)$.
Doprowadzenie tak naszkicowanego programu do końca pozwoliłoby
na podanie pełnej klasyfikacji dodatnich odwzorowań bistochastycznych
na algebrze $M_3$ i z pewnością przyniosłoby wymierne korzyści w
postaci zastosowań do kwantowej teorii informacji.
% * * *
